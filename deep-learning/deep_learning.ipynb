{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjMrREk4Vit0"
      },
      "source": [
        "### Colab Setup (Don't run this cell if you're not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYQswrKJVit3",
        "outputId": "46eb3f18-e310-4419-d262-665a7b4cf1e4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/google-drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn3ndin4Vit4"
      },
      "outputs": [],
      "source": [
        "movie_titles_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/checkpoints/merge4.csv'\n",
        "combined_data_1_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-mXf6aVit4"
      },
      "source": [
        "### Non-Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pOVKd0kAVit4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "movie_titles_path = '../prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '../IMDB_data/merge4.csv'\n",
        "combined_data_1_path = '../prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm/Bellkor'\n",
        "google_save_path = os.path.expanduser('~/google-drive/CSC 422/CSC422 Class Project/codes/checkpoints/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### How to attach google drive to filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 file:/var/cuda-repo-10-1-local-10.1.105-418.39  InRelease\n",
            "Ign:1 file:/var/cuda-repo-10-1-local-10.1.105-418.39  InRelease\n",
            "Get:2 file:/var/cuda-repo-10-1-local-10.1.105-418.39  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-10-1-local-10.1.105-418.39  Release [574 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
            "Hit:4 http://us.archive.ubuntu.com/ubuntu bionic InRelease                     \n",
            "Get:5 http://us.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]   \n",
            "Get:7 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:8 http://us.archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB] \n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease          \n",
            "Hit:10 http://ppa.launchpad.net/openafs/stable/ubuntu bionic InRelease         \n",
            "Get:11 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main i386 Packages [1,356 B]\n",
            "Get:12 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 Packages [1,368 B]\n",
            "Get:13 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main Translation-en [1,004 B]\n",
            "Fetched 280 kB in 2s (115 kB/s)                            \n",
            "Reading package lists... Done\n",
            "Get:1 file:/var/cuda-repo-10-1-local-10.1.105-418.39  InRelease\n",
            "Ign:1 file:/var/cuda-repo-10-1-local-10.1.105-418.39  InRelease\n",
            "Get:2 file:/var/cuda-repo-10-1-local-10.1.105-418.39  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-10-1-local-10.1.105-418.39  Release [574 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \u001b[0m\u001b[33m\u001b[33m\n",
            "Hit:4 http://us.archive.ubuntu.com/ubuntu bionic InRelease                     \u001b[0m\n",
            "Get:5 http://us.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]   \u001b[0m\n",
            "Hit:6 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease   \u001b[0m\u001b[33m\n",
            "Get:7 http://us.archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB] \u001b[0m\u001b[33m\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease         \u001b[0mm\u001b[33m\u001b[33m\n",
            "Hit:10 http://ppa.launchpad.net/openafs/stable/ubuntu bionic InRelease         \u001b[0m\n",
            "Fetched 261 kB in 2s (108 kB/s)[0m                                             \u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "208 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  google-drive-ocamlfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 208 not upgraded.\n",
            "Need to get 1,330 kB of archives.\n",
            "After this operation, 7,023 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic/main amd64 google-drive-ocamlfuse amd64 0.7.27-0ubuntu1~ubuntu18.04.1 [1,330 kB]\n",
            "Fetched 1,330 kB in 4s (376 kB/s)                   \u001b[0m3m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 278062 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
          ]
        }
      ],
      "source": [
        "!sudo add-apt-repository ppa:alessandro-strada/ppa -y\n",
        "!sudo apt update && sudo apt install google-drive-ocamlfuse -y\n",
        "!mkdir ~/.gdfuse\n",
        "!mkdir ~/.gdfuse/default\n",
        "!cat utils/gdfuse-config > ~/.gdfuse/default/config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/gmbaker3/.gdfuse’: File exists\n",
            "mkdir: cannot create directory ‘/home/gmbaker3/.gdfuse/default’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.gdfuse\n",
        "!mkdir ~/.gdfuse/default\n",
        "!cat utils/gdfuse-config > ~/.gdfuse/default/config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the next command in the terminal and sign in to your google account.\n",
        "The key will be in the url that your browser redirects to after sign in. Copy the key and paste it in the terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'google-drive-ocamlfuse -headless'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir ~/google-drive\n",
        "!google-drive-ocamlfuse ~/google-drive"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce RTX 2080 Ti'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJYtb7CKVit5",
        "outputId": "e249f71c-e6d4-4726-ee35-74b534970fff"
      },
      "outputs": [],
      "source": [
        "# See the GPU specs\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEJu3DbPVit5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81ecc38ecc2650b81c042a385599a3af31b4e1e6",
        "id": "NU1QuoEWw5jR"
      },
      "source": [
        "### Load Movie Tiles w/o metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "a698fdfcf9ac8ef2193c3b40503b92283c5bec8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "-lSOFdGKw5jS",
        "outputId": "5ff913d9-283c-4828-dd82-371410f924fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape Movie-Titles:\t(17770, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5988</th>\n",
              "      <td>2001.0</td>\n",
              "      <td>The First Year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5556</th>\n",
              "      <td>1991.0</td>\n",
              "      <td>Silk Stalkings: Season 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5610</th>\n",
              "      <td>1961.0</td>\n",
              "      <td>The Bruce Brown Surf Collection: Surfing Hollo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>2004.0</td>\n",
              "      <td>Dream Theater: Live at Budokan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1769</th>\n",
              "      <td>1988.0</td>\n",
              "      <td>The Incredible Hulk Returns</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Year                                               Name\n",
              "Id                                                             \n",
              "5988  2001.0                                     The First Year\n",
              "5556  1991.0                           Silk Stalkings: Season 1\n",
              "5610  1961.0  The Bruce Brown Surf Collection: Surfing Hollo...\n",
              "1491  2004.0                     Dream Theater: Live at Budokan\n",
              "1769  1988.0                        The Incredible Hulk Returns"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "import re\n",
        "\n",
        "for_pd = StringIO()\n",
        "with open(movie_titles_path, encoding = 'ISO-8859-1') as movie_titles:\n",
        "    for line in movie_titles:\n",
        "        new_line = re.sub(r',', '|', line.rstrip(), count=2)\n",
        "        print (new_line, file=for_pd)\n",
        "\n",
        "for_pd.seek(0)\n",
        "\n",
        "movie_titles = pd.read_csv(for_pd, sep='|', header=None, names=['Id', 'Year', 'Name']).set_index('Id')\n",
        "del for_pd\n",
        "\n",
        "print('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\n",
        "movie_titles.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c0612cd9da11d6c8a984db24f2befec720f8cdbc",
        "id": "DAKkIpSaw5jS"
      },
      "source": [
        "### Load Movie Titles w/ metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "HOzmfXNFw5jS",
        "outputId": "c940c357-6104-48ad-fe99-ec07334887b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of missing values in each column:\n",
            "MovieID                      0\n",
            "Year                         3\n",
            "AggregateAverageRating     789\n",
            "NumRating                  789\n",
            "Directors                 1972\n",
            "creators                  2232\n",
            "Genre                       97\n",
            "Keywords                  1190\n",
            "description                914\n",
            "duration                  2075\n",
            "actors                     327\n",
            "contentRating             2947\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "movie_metadata_raw = pd.read_csv(movie_metadata_path)\n",
        "\n",
        "movie_metadata = movie_metadata_raw[movie_metadata_raw['imdbID'].notnull()].set_index('Name').drop('imdbID', axis=1)\n",
        "del movie_metadata_raw\n",
        "\n",
        "na_count = movie_metadata.isna().sum()\n",
        "print('Number of missing values in each column:\\n{}'.format(na_count))\n",
        "\n",
        "movie_metadata = movie_metadata.set_index('MovieID').sort_index()\n",
        "# movie_metadata.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>AggregateAverageRating</th>\n",
              "      <th>NumRating</th>\n",
              "      <th>duration</th>\n",
              "      <th>contentRating</th>\n",
              "      <th>Genre_</th>\n",
              "      <th>Genre_Action</th>\n",
              "      <th>Genre_Adventure</th>\n",
              "      <th>Genre_Animation</th>\n",
              "      <th>Genre_Biography</th>\n",
              "      <th>...</th>\n",
              "      <th>description_work</th>\n",
              "      <th>description_working</th>\n",
              "      <th>description_world</th>\n",
              "      <th>description_writer</th>\n",
              "      <th>description_year</th>\n",
              "      <th>description_years</th>\n",
              "      <th>description_york</th>\n",
              "      <th>description_you</th>\n",
              "      <th>description_young</th>\n",
              "      <th>description_zone</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MovieID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003.0</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>531.000000</td>\n",
              "      <td>100.309953</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.194663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166875</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1997.0</td>\n",
              "      <td>7.700000</td>\n",
              "      <td>11026.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>6.518252</td>\n",
              "      <td>33629.363276</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2004.0</td>\n",
              "      <td>6.518252</td>\n",
              "      <td>33629.363276</td>\n",
              "      <td>360.000000</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1997.0</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>1680.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.268193</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1034 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Year  AggregateAverageRating     NumRating    duration   \n",
              "MovieID                                                             \n",
              "1        2003.0                7.700000    531.000000  100.309953  \\\n",
              "3        1997.0                7.700000  11026.000000  122.000000   \n",
              "4        1994.0                6.518252  33629.363276   35.000000   \n",
              "5        2004.0                6.518252  33629.363276  360.000000   \n",
              "6        1997.0                7.500000   1680.000000   90.000000   \n",
              "\n",
              "         contentRating  Genre_  Genre_Action  Genre_Adventure   \n",
              "MovieID                                                         \n",
              "1                   12       0             0                0  \\\n",
              "3                   16       0             0                0   \n",
              "4                   16       0             0                0   \n",
              "5                   21       0             0                0   \n",
              "6                   12       0             0                0   \n",
              "\n",
              "         Genre_Animation  Genre_Biography  ...  description_work   \n",
              "MovieID                                    ...                     \n",
              "1                      1                0  ...               0.0  \\\n",
              "3                      0                0  ...               0.0   \n",
              "4                      0                0  ...               0.0   \n",
              "5                      0                0  ...               0.0   \n",
              "6                      0                0  ...               0.0   \n",
              "\n",
              "         description_working  description_world  description_writer   \n",
              "MovieID                                                               \n",
              "1                        0.0           0.194663                 0.0  \\\n",
              "3                        0.0           0.000000                 0.0   \n",
              "4                        0.0           0.000000                 0.0   \n",
              "5                        0.0           0.000000                 0.0   \n",
              "6                        0.0           0.000000                 0.0   \n",
              "\n",
              "         description_year  description_years  description_york   \n",
              "MovieID                                                          \n",
              "1                     0.0                0.0               0.0  \\\n",
              "3                     0.0                0.0               0.0   \n",
              "4                     0.0                0.0               0.0   \n",
              "5                     0.0                0.0               0.0   \n",
              "6                     0.0                0.0               0.0   \n",
              "\n",
              "         description_you  description_young  description_zone  \n",
              "MovieID                                                        \n",
              "1                    0.0           0.166875               0.0  \n",
              "3                    0.0           0.000000               0.0  \n",
              "4                    0.0           0.000000               0.0  \n",
              "5                    0.0           0.000000               0.0  \n",
              "6                    0.0           0.268193               0.0  \n",
              "\n",
              "[5 rows x 1034 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Impute missing values using the mean for continuous variables\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "movie_metadata[['Year', 'NumRating', 'duration', 'AggregateAverageRating']] = mean_imputer.fit_transform(movie_metadata[['Year', 'NumRating', 'duration', 'AggregateAverageRating']])\n",
        "\n",
        "# Fill missing values in 'ContentRating' with the mode value\n",
        "mode_content_rating = movie_metadata['contentRating'].mode().iloc[0]\n",
        "movie_metadata['contentRating'].fillna(mode_content_rating, inplace=True)\n",
        "\n",
        "# Encode 'ContentRating' column using LabelEncoder\n",
        "le_content_rating = LabelEncoder()\n",
        "movie_metadata['contentRating'] = le_content_rating.fit_transform(movie_metadata['contentRating'])\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "movie_metadata['Genre'].fillna('', inplace=True)\n",
        "movie_metadata['actors'].fillna('', inplace=True)\n",
        "movie_metadata['Directors'].fillna('', inplace=True)\n",
        "movie_metadata['creators'].fillna('', inplace=True)\n",
        "movie_metadata['Keywords'].fillna('', inplace=True)\n",
        "movie_metadata['description'].fillna('', inplace=True)\n",
        "\n",
        "# Split comma-separated values into lists\n",
        "movie_metadata['Genre'] = movie_metadata['Genre'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['actors'] = movie_metadata['actors'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['Directors'] = movie_metadata['Directors'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['creators'] = movie_metadata['creators'].apply(lambda x: str(x).split(', '))\n",
        "\n",
        "# Encode multi-value columns using MultiLabelBinarizer\n",
        "mlb_encoders = {}\n",
        "multi_label_columns = ['Genre', 'actors', 'Directors', 'creators']\n",
        "min_rows_with_1 = int(movie_metadata.shape[0] * 0.005)\n",
        "for col in multi_label_columns:\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    encoded_col = mlb.fit_transform(movie_metadata[col])\n",
        "    encoded_df = pd.DataFrame(encoded_col, columns=[f\"{col}_{c}\" for c in mlb.classes_], index=movie_metadata.index)\n",
        "    \n",
        "    # Filter columns with at least 100 rows containing a 1\n",
        "    cols_to_keep = encoded_df.columns[encoded_df.sum(axis=0) >= min_rows_with_1]\n",
        "    encoded_df = encoded_df[cols_to_keep]\n",
        "\n",
        "    movie_metadata = pd.concat([movie_metadata.drop(col, axis=1), encoded_df], axis=1)\n",
        "    mlb_encoders[col] = mlb\n",
        "\n",
        "# Encode text columns using TfidfVectorizer\n",
        "tfidf_encoders = {}\n",
        "max_features = 500\n",
        "text_columns = ['Keywords', 'description']\n",
        "for col in text_columns:\n",
        "    tfidf = TfidfVectorizer(max_features=max_features)\n",
        "    encoded_col = tfidf.fit_transform(movie_metadata[col].astype(str)).toarray()\n",
        "    encoded_df = pd.DataFrame(encoded_col, columns=[f\"{col}_{c}\" for c in tfidf.get_feature_names_out()], index=movie_metadata.index)\n",
        "    movie_metadata = pd.concat([movie_metadata.drop(col, axis=1), encoded_df], axis=1)\n",
        "    tfidf_encoders[col] = tfidf\n",
        "\n",
        "movie_metadata.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity ratio: 97.65%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a dense PyTorch tensor\n",
        "dense_tensor = torch.from_numpy(movie_metadata.to_numpy())\n",
        "\n",
        "# Calculate the number of non-zero elements in the tensor\n",
        "num_nonzero = torch.nonzero(dense_tensor).size(0)\n",
        "\n",
        "# Calculate the total number of elements in the tensor\n",
        "total_elements = dense_tensor.numel()\n",
        "\n",
        "# Calculate the sparsity ratio\n",
        "sparsity_ratio = 1.0 - (num_nonzero / total_elements)\n",
        "\n",
        "# Print the sparsity ratio\n",
        "print(\"Sparsity ratio: {:.2f}%\".format(sparsity_ratio * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3e5992741a799555bb326b04cdbb85fda14598ee",
        "id": "-UyJ3DKmw5jS"
      },
      "source": [
        "### Load user-data structure (1/4 to save memory + speed up compute) and preprocess to extract all rating to form a matrix. File structure is messy mix of json and csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "cf6473e25f7fd85d4896e1a87fd92b51f26fafa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "vuOM31sPw5jS",
        "outputId": "58cc4bd4-3197-4094-b1f3-ade46cb8d713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape User-Ratings:\t(24053764, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11854615</th>\n",
              "      <td>2107905</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2005-09-11</td>\n",
              "      <td>2290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3300800</th>\n",
              "      <td>437732</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2005-07-25</td>\n",
              "      <td>607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16076624</th>\n",
              "      <td>1714427</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2005-02-23</td>\n",
              "      <td>3113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627372</th>\n",
              "      <td>1084694</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2005-06-12</td>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10202975</th>\n",
              "      <td>1236224</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2005-10-14</td>\n",
              "      <td>1975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             User  Rating        Date  Movie\n",
              "11854615  2107905     5.0  2005-09-11   2290\n",
              "3300800    437732     4.0  2005-07-25    607\n",
              "16076624  1714427     3.0  2005-02-23   3113\n",
              "627372    1084694     4.0  2005-06-12    175\n",
              "10202975  1236224     2.0  2005-10-14   1975"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import deque\n",
        "\n",
        "# Load single data-file\n",
        "df_raw = pd.read_csv(combined_data_1_path, header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
        "\n",
        "\n",
        "# Find empty rows to slice dataframe for each movie\n",
        "tmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\n",
        "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
        "\n",
        "# Shift the movie_indices by one to get start and endpoints of all movies\n",
        "shifted_movie_indices = deque(movie_indices)\n",
        "shifted_movie_indices.rotate(-1)\n",
        "\n",
        "\n",
        "# Gather all dataframes\n",
        "user_data = []\n",
        "\n",
        "# Iterate over all movies\n",
        "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
        "    \n",
        "    # Check if it is the last movie in the file\n",
        "    if df_id_1<df_id_2:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
        "    else:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
        "        \n",
        "    # Create movie_id column\n",
        "    tmp_df['Movie'] = movie_id\n",
        "    \n",
        "    # Append dataframe to list\n",
        "    user_data.append(tmp_df)\n",
        "\n",
        "# Combine all dataframes\n",
        "df = pd.concat(user_data)\n",
        "del user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
        "print('Shape User-Ratings:\\t{}'.format(df.shape))\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsAAgCpVit7"
      },
      "source": [
        "#### More formatting for user-data and only use X of the users (choose users with the most ratings) from (1/4) of the total data. Number subject to change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "sDxd-T17Vit7",
        "outputId": "057a4167-50f0-4fc5-e9db-5628371f0eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique users:\t470758\n",
            "Number of unique movies:\t4499\n",
            "Number users: 4707\n",
            "Number movies: 4499\n",
            "Shape: (2197744, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>User</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2002-04-12</td>\n",
              "      <td>1143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2002-05-03</td>\n",
              "      <td>762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2002-10-10</td>\n",
              "      <td>1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2003-07-01</td>\n",
              "      <td>2469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2005-02-06</td>\n",
              "      <td>3550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rating        Date  Movie\n",
              "User                           \n",
              "0        3.0  2002-04-12   1143\n",
              "0        3.0  2002-05-03    762\n",
              "0        3.0  2002-10-10   1999\n",
              "0        3.0  2003-07-01   2469\n",
              "0        3.0  2005-02-06   3550"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_movies = df['Movie'].nunique()\n",
        "unique_users = df['User'].nunique()\n",
        "\n",
        "print(f'Number of unique users:\\t{unique_users}')\n",
        "print(f'Number of unique movies:\\t{unique_movies}')\n",
        "\n",
        "pct_movies = unique_movies\n",
        "pct_users = int(unique_users * 0.01)\n",
        "\n",
        "filter_movies = df['Movie'].value_counts().sort_values(ascending=False)[:pct_movies].index\n",
        "\n",
        "filter_users = df['User'].value_counts().sort_values(ascending=False)[:pct_users].index\n",
        "\n",
        "df_filtered = df[df[\"Movie\"].isin(filter_movies) & df[\"User\"].isin(filter_users)]\n",
        "del filter_movies, filter_users, df\n",
        "\n",
        "# rename the users and movies with new ids start from 0\n",
        "df_filtered['User'] = df_filtered['User'].astype(\"category\")\n",
        "df_filtered['Movie'] = df_filtered['Movie'].astype(\"category\")\n",
        "df_filtered['User'] = df_filtered['User'].cat.codes.values\n",
        "df_filtered['Movie'] = df_filtered['Movie'].cat.codes.values\n",
        "\n",
        "# make user the index and sort the index\n",
        "df_filtered.set_index('User', inplace=True)\n",
        "df_filtered.sort_index(inplace=True)\n",
        "\n",
        "print(f'Number users: {df_filtered.index.nunique()}')\n",
        "print(f'Number movies: {df_filtered[\"Movie\"].nunique()}')\n",
        "print(f'Shape: {df_filtered.shape}')\n",
        "df_filtered.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717sgV1vVit7"
      },
      "source": [
        "### Shuffle the filtered dataframe and split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5d286e3c457b425123827ccd8ab50eb62cf83530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "KuMjQjQxw5jU",
        "outputId": "21b438bf-0a99-4b7e-f976-d195a87d02bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1865</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2003-06-06</td>\n",
              "      <td>1393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3045</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2003-05-04</td>\n",
              "      <td>4368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>578</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2004-02-08</td>\n",
              "      <td>1254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3597</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2003-08-13</td>\n",
              "      <td>2492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3613</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2005-01-02</td>\n",
              "      <td>699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1261</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2005-05-06</td>\n",
              "      <td>2912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2723</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2004-08-26</td>\n",
              "      <td>2171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2766</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2004-03-01</td>\n",
              "      <td>3253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>187</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2005-05-08</td>\n",
              "      <td>3937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4439</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2004-12-27</td>\n",
              "      <td>2781</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User  Rating        Date  Movie\n",
              "0  1865     2.0  2003-06-06   1393\n",
              "1  3045     2.0  2003-05-04   4368\n",
              "2   578     2.0  2004-02-08   1254\n",
              "3  3597     4.0  2003-08-13   2492\n",
              "4  3613     4.0  2005-01-02    699\n",
              "5  1261     4.0  2005-05-06   2912\n",
              "6  2723     5.0  2004-08-26   2171\n",
              "7  2766     2.0  2004-03-01   3253\n",
              "8   187     3.0  2005-05-08   3937\n",
              "9  4439     5.0  2004-12-27   2781"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Shuffle DataFrame\n",
        "\n",
        "# create random seed\n",
        "import random\n",
        "seed = random.seed(42)\n",
        "\n",
        "# Shuffle DataFrame\n",
        "df_filtered = df_filtered.sample(frac=1, random_state=seed).reset_index()\n",
        "\n",
        "percent_test = .2\n",
        "\n",
        "\n",
        "# Split train and set set based on percentage\n",
        "df_train = df_filtered.sample(frac=1-percent_test, random_state=seed).reset_index(drop=True)\n",
        "df_test = df_filtered.drop(df_train.index).reset_index(drop=True)\n",
        "\n",
        "# split into X and y\n",
        "X_train = df_train.drop('Rating', axis=1)\n",
        "y_train = df_train['Rating']\n",
        "\n",
        "X_test = df_test.drop('Rating', axis=1)\n",
        "y_test = df_test['Rating']\n",
        "\n",
        "df_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHPD7RCVit7"
      },
      "source": [
        "## Baseline Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPbuqbaVit7"
      },
      "source": [
        "### Bellkor Algorithm \n",
        "Uses library from https://github.com/dandxy89/BellkorAlgorithm<br>\n",
        "Based on paper https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-m0R7NVit8"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nt358iwVit8",
        "outputId": "d1176348-3a42-4b5d-8c4c-4906ef0cb9f8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dandxy89/BellkorAlgorithm\n",
        "#!pip install -r {bellkor_requirements_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGsBJYP0Vit8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f'/content/{bellkor_import_path}')\n",
        "from Bellkor.Algorithm import BellkorAlgorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taCYCFAVit8"
      },
      "source": [
        "##### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RpEGpKVVit8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "df_train_timestamp = df_train.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_train_timestamp['timestamp'] = pd.to_datetime(df_train['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_train_timestamp[\"timestamp\"].min()\n",
        "end_time = df_train_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_train_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_train_timestamp[\"User\"].nunique()\n",
        "global_mean = df_train_timestamp[\"Rating\"].mean()\n",
        "average_df = df_train_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWVx8HO7Vit8"
      },
      "outputs": [],
      "source": [
        "calibrator = BellkorAlgorithm(n_items=movie_count, \n",
        "                              n_users=user_count, \n",
        "                              global_mean=global_mean,\n",
        "                              time_setting=dict(Start=adjusted_start_day, \n",
        "                                                End=adjusted_end_day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GIJ6bh3Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_train_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Ag8P0JVit8"
      },
      "outputs": [],
      "source": [
        "# TODO: increase sample size and #iterations for algorithm\n",
        "# TODO: Probably need to run a script to auto optimize parameters on this\n",
        "cost, error = calibrator.train(x=X, average_times=average_times, sample_size=100, iterations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATuW4yegVit8"
      },
      "source": [
        "##### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6_6s57pVit8"
      },
      "outputs": [],
      "source": [
        "# calc average_times and X for test data\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "df_test_timestamp = df_test.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_test_timestamp['timestamp'] = pd.to_datetime(df_filtered['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_test_timestamp[\"timestamp\"].min()\n",
        "end_time = df_test_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_test_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_test_timestamp[\"User\"].nunique()\n",
        "global_mean = df_test_timestamp[\"Rating\"].mean()\n",
        "average_df = df_test_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P25g-y7Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_test_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGMqBC0SVit9"
      },
      "outputs": [],
      "source": [
        "preds = calibrator.predict(x=X, average_times=average_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3vShyVZVit9"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def evaluate(preds, y_test):\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(preds.values, y_test.values))\n",
        "    print(\"RMSE: {}\".format(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjrm_oLAVit9"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame(data=preds, columns=[\"Index\", \"Prediction\"])\n",
        "predictions.head(n=10)\n",
        "\n",
        "# convert predictions to series\n",
        "predictions = pd.Series(data=predictions[\"Prediction\"].values, index=predictions[\"Index\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVS1tgC9Vit9",
        "outputId": "31c95c41-9232-4627-eda2-09ab3ae940b9"
      },
      "outputs": [],
      "source": [
        "evaluate(predictions, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQ0kyyMVit-"
      },
      "source": [
        "## Machine Learning Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filtered = df_filtered.drop('Date', axis=1)\n",
        "X_train = X_train.drop('Date', axis=1)\n",
        "X_test = X_test.drop('Date', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pi0qTFYVit-"
      },
      "source": [
        "### Matrix Factorization (Dot Product) w/ hidden layers\n",
        "Uses embeddings to represent users and movies. The dot product of user embeddings (n_users x e_dims) and movie embedding matrix (n_movies x e_dims) is a good approx of rating from user to movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCi-Z24pVit-"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, users, movie_ids, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.int)\n",
        "        self.movies = torch.tensor(movie_ids, dtype=torch.int)\n",
        "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
        "\n",
        "class RecommenderModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, embedding_size):\n",
        "        super(RecommenderModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(n_movies, embedding_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(2 * embedding_size + 1, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        user_vector = self.user_embedding(user).squeeze(1)\n",
        "        movie_vector = self.movie_embedding(movie).squeeze(1)\n",
        "        # mul and sum are needed because dot only works with 1D tensors\n",
        "        x = torch.mul(user_vector, movie_vector).sum(1).unsqueeze(1)\n",
        "        cat = torch.cat([user_vector, movie_vector, x], dim=1)\n",
        "        dense = torch.relu(self.fc1(cat))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc2(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc3(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc4(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        y = self.fc5(dense)\n",
        "        return y.flatten()\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for user, movie, rating in dataloader:\n",
        "        if type(movie) is dict:\n",
        "            movie = {key: value.to(device) for key, value in movie.items()}\n",
        "        else:\n",
        "            movie = movie.to(device)\n",
        "        user, rating = user.to(device), rating.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(user, movie)\n",
        "        loss = criterion(outputs, rating)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * user.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                if type(movie) is dict:\n",
        "                    movie = {key: value.to(device) for key, value in movie.items()}\n",
        "                else:\n",
        "                    movie = movie.to(device)\n",
        "                user, rating = user.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                loss = criterion(outputs, rating)\n",
        "                running_loss += loss.item() * user.size(0)\n",
        "        return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        ground_truth = []\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                if type(movie) is dict:\n",
        "                    movie = {key: value.to(device) for key, value in movie.items()}\n",
        "                else:\n",
        "                    movie = movie.to(device)\n",
        "                user, rating = user.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                predictions.extend(outputs.view(-1).cpu().numpy())\n",
        "                ground_truth.extend(rating.view(-1).cpu().numpy())\n",
        "        return np.sqrt(mean_squared_error(ground_truth, predictions))\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, model_name, restore_state=False):\n",
        "    if restore_state:\n",
        "        checkpoint = torch.load(f\"{google_save_path}/{model_name}-checkpoint.pt\")\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        model.load_state_dict(checkpoint['model_state_dict']).to(device)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    bad_epochs = 0\n",
        "\n",
        "    print(\"Starting Training...\")\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        val_loss = validate(model, val_dataloader, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"{google_save_path}/{model_name}-best.pt\")\n",
        "            bad_epochs = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= 5:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        # save the model checkpoint\n",
        "        if epoch % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': val_loss\n",
        "            }, f\"{google_save_path}/{model_name}-checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OcoMPtH_Vit_",
        "outputId": "9ab02567-1a58-4017-e697-4b62599d7ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m val_dataloader \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m---> 19\u001b[0m training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \u001b[39m\"\u001b[39;49m\u001b[39mnn-dot\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[27], line 101\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, model_name, restore_state)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting Training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, n_epochs):\n\u001b[0;32m--> 101\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001b[1;32m    102\u001b[0m     val_loss \u001b[39m=\u001b[39m validate(model, val_dataloader, criterion, device)\n\u001b[1;32m    103\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[27], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     51\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 52\u001b[0m \u001b[39mfor\u001b[39;00m user, movie, rating \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     53\u001b[0m     user, movie, rating \u001b[39m=\u001b[39m user\u001b[39m.\u001b[39mto(device), movie\u001b[39m.\u001b[39mto(device), rating\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training\n",
        "n_users = df_filtered['User'].nunique()\n",
        "n_movies = df_filtered['Movie'].nunique()\n",
        "embedding_size = 20\n",
        "batch_size = 1024\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RecommenderModel(n_users, n_movies, embedding_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_dataset = MovieDataset(X_train['User'], X_train['Movie'], y_train)\n",
        "val_dataset = MovieDataset(X_test['User'], X_test['Movie'], y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \"nn-dot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only used if we decide to do hyperparameter tuning\n",
        "# The user dataset would need to be split into three parts\n",
        "# Evaluation\n",
        "test_dataset = MovieDataset(X_test['User'], X_test['Movie'], y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(f\"{google_save_path}/nn-dot.pt\"))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "rmse = evaluate(model, test_dataloader, device)\n",
        "print(f\"Test RMSE: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: REMOVE OR USE\n",
        "\n",
        "# Making predictions\n",
        "# def predict(model, user, movie, device):\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         user_tensor = torch.tensor([user], dtype=torch.long, device=device).unsqueeze(0)\n",
        "#         movie_tensor = torch.tensor([movie], dtype=torch.long, device=device).unsqueeze(0)\n",
        "#         output = model(user_tensor, movie_tensor)\n",
        "#         return output.item()\n",
        "\n",
        "# user_id = 1\n",
        "# movie_id = 100\n",
        "\n",
        "#prediction = predict(model, user_id, movie_id, device)\n",
        "#print(f\"Predicted rating for user {user_id} and movie {movie_id}: {prediction}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DpXmsGjNViuA"
      },
      "source": [
        "### The Deep Hybrid System with Metadata\n",
        "Uses movie metadata to improve recommendations. Metadata is combined with embeddings of user-id and movie-id."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDHjXoVViuB"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class HybridMovieDataset(Dataset):\n",
        "    def __init__(self, users, movie_ids, movie_metadata, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.int)\n",
        "        self.movie_ids = torch.tensor(movie_ids, dtype=torch.int)\n",
        "        self.movie_metadata = torch.tensor(movie_metadata, dtype=torch.float)\n",
        "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], {\"id\": self.movie_ids[idx], \"metadata\": self.movie_metadata[self.movie_ids[idx]]}, self.ratings[idx]\n",
        "\n",
        "class HybridRecommenderModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, embedding_size, metadata_size):\n",
        "        super(HybridRecommenderModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(n_movies, embedding_size)\n",
        "        self.movie_metadata = nn.Linear(metadata_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(3 * embedding_size + 1, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.m0 = nn.Linear(embedding_size, 256)\n",
        "        self.m1 = nn.Linear(128, embedding_size)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        user_vector = self.user_embedding(user).squeeze(1)\n",
        "        movie_vector = self.movie_embedding(movie[\"id\"]).squeeze(1)\n",
        "        metadata_vector = torch.relu(self.movie_metadata(movie[\"metadata\"]))\n",
        "\n",
        "        # metadata_vector = torch.relu(self.m0(metadata_vector))\n",
        "        # metadata_vector = self.dropout(metadata_vector)\n",
        "        # metadata_vector = torch.relu(self.fc2(metadata_vector))\n",
        "        # metadata_vector = self.dropout(metadata_vector)\n",
        "        # metadata_vector = self.m1(metadata_vector)\n",
        "        # metadata_vector_norm = nn.functional.normalize(metadata_vector, p=2, dim=1)\n",
        "\n",
        "        # mul and sum are needed because dot only works with 1D tensors\n",
        "        metadata_interaction = torch.mul(user_vector, metadata_vector)\n",
        "        x = torch.mul(user_vector, movie_vector).sum(1).unsqueeze(1)\n",
        "        cat = torch.cat([user_vector, movie_vector, metadata_interaction, x], dim=1)\n",
        "        dense = torch.relu(self.fc1(cat))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc2(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc3(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc4(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        y = self.fc5(dense)\n",
        "        return y.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m val_dataloader \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[0;32m---> 21\u001b[0m training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \u001b[39m\"\u001b[39;49m\u001b[39mnn-hybrid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[13], line 113\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, model_name, restore_state)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting Training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, n_epochs):\n\u001b[0;32m--> 113\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001b[1;32m    114\u001b[0m     val_loss \u001b[39m=\u001b[39m validate(model, val_dataloader, criterion, device)\n\u001b[1;32m    115\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m - Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[13], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m user, rating \u001b[39m=\u001b[39m user\u001b[39m.\u001b[39mto(device), rating\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 59\u001b[0m outputs \u001b[39m=\u001b[39m model(user, movie)\n\u001b[1;32m     60\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, rating)\n\u001b[1;32m     61\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[18], line 52\u001b[0m, in \u001b[0;36mHybridRecommenderModel.forward\u001b[0;34m(self, user, movie)\u001b[0m\n\u001b[1;32m     50\u001b[0m cat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([user_vector, movie_vector, metadata_interaction, x], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m dense \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(cat))\n\u001b[0;32m---> 52\u001b[0m dense \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(dense)\n\u001b[1;32m     53\u001b[0m dense \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(dense))\n\u001b[1;32m     54\u001b[0m dense \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(dense)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training\n",
        "n_users = df_filtered['User'].nunique()\n",
        "n_movies = df_filtered['Movie'].nunique()\n",
        "embedding_size = 20 #100\n",
        "batch_size = 1 #2048\n",
        "# get the columns of the metadata\n",
        "n_features_metadata = movie_metadata.shape[1]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = HybridRecommenderModel(n_users, n_movies, embedding_size, n_features_metadata).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_dataset = HybridMovieDataset(X_train['User'], X_train['Movie'], movie_metadata.values, y_train)\n",
        "val_dataset = HybridMovieDataset(X_test['User'], X_test['Movie'], movie_metadata.values, y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \"nn-hybrid\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
