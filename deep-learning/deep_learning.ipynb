{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjMrREk4Vit0"
      },
      "source": [
        "### Colab Setup (Don't run this cell if you're not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYQswrKJVit3",
        "outputId": "46eb3f18-e310-4419-d262-665a7b4cf1e4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/google-drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn3ndin4Vit4"
      },
      "outputs": [],
      "source": [
        "movie_titles_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/checkpoints/merge4.csv'\n",
        "combined_data_1_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-mXf6aVit4"
      },
      "source": [
        "### Non-Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOVKd0kAVit4"
      },
      "outputs": [],
      "source": [
        "movie_titles_path = '../prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '../IMDB_data/merge4.csv'\n",
        "combined_data_1_path = '../prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm/Bellkor'\n",
        "google_save_path = '/content/google-drive/MyDrive/CSC 422/CSC422 Class Project/codes/checkpoints/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### How to attach google drive to filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo add-apt-repository ppa:alessandro-strada/ppa -y\n",
        "!sudo apt update && sudo apt install google-drive-ocamlfuse -y\n",
        "!mkdir ~/.gdfuse/default\n",
        "!cat utils/gdfuse-config > ~/.gdfuse/default/config"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the next command in the terminal and sign in to your google account.\n",
        "The key will be in the url that your browser redirects to after sign in. Copy the key and paste it in the terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'google-drive-ocamlfuse -headless'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir ~/google-drive\n",
        "!google-drive-ocamlfuse ~/google-drive"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJYtb7CKVit5",
        "outputId": "e249f71c-e6d4-4726-ee35-74b534970fff"
      },
      "outputs": [],
      "source": [
        "# See the GPU specs\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "owNUR4Jzw5jQ"
      },
      "outputs": [],
      "source": [
        "# TODO: REMOVE THIS AFTER TESTING\n",
        "\n",
        "# To store the data\n",
        "#import pandas as pd\n",
        "\n",
        "# To do linear algebra\n",
        "#import numpy as np\n",
        "\n",
        "# To create plots\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# To create interactive plots\n",
        "#import nbformat\n",
        "#from plotly.offline import init_notebook_mode, plot, iplot\n",
        "#import plotly.graph_objs as go\n",
        "#init_notebook_mode(connected=True)\n",
        "\n",
        "# To compute similarities between vectors\n",
        "#from sklearn.metrics import mean_squared_error\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "##from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# To create sparse matrices\n",
        "#from scipy.sparse import coo_matrix\n",
        "\n",
        "# To stack sparse matrices\n",
        "#from scipy.sparse import vstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEJu3DbPVit5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81ecc38ecc2650b81c042a385599a3af31b4e1e6",
        "id": "NU1QuoEWw5jR"
      },
      "source": [
        "### Load Movie Tiles w/o metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "a698fdfcf9ac8ef2193c3b40503b92283c5bec8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "-lSOFdGKw5jS",
        "outputId": "5ff913d9-283c-4828-dd82-371410f924fd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "import re\n",
        "\n",
        "for_pd = StringIO()\n",
        "with open(movie_titles_path, encoding = 'ISO-8859-1') as movie_titles:\n",
        "    for line in movie_titles:\n",
        "        new_line = re.sub(r',', '|', line.rstrip(), count=2)\n",
        "        print (new_line, file=for_pd)\n",
        "\n",
        "for_pd.seek(0)\n",
        "\n",
        "movie_titles = pd.read_csv(for_pd, sep='|', header=None, names=['Id', 'Year', 'Name']).set_index('Id')\n",
        "del for_pd\n",
        "\n",
        "print('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\n",
        "movie_titles.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c0612cd9da11d6c8a984db24f2befec720f8cdbc",
        "id": "DAKkIpSaw5jS"
      },
      "source": [
        "### Load Movie Titles w/ metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "HOzmfXNFw5jS",
        "outputId": "c940c357-6104-48ad-fe99-ec07334887b8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "movie_metadata_raw = pd.read_csv(movie_metadata_path)\n",
        "\n",
        "movie_metadata = movie_metadata_raw[movie_metadata_raw['imdbID'].notnull()].set_index('Name').drop('imdbID', axis=1)\n",
        "del movie_metadata_raw\n",
        "\n",
        "na_count = movie_metadata.isna().sum()\n",
        "print('Number of missing values in each column:\\n{}'.format(na_count))\n",
        "\n",
        "movie_metadata = movie_metadata.set_index('MovieID').sort_index()\n",
        "# movie_metadata.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Impute missing values using the mean for continuous variables\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "movie_metadata[['Year', 'NumRating', 'duration', 'AggregateAverageRating']] = mean_imputer.fit_transform(movie_metadata[['Year', 'NumRating', 'duration', 'AggregateAverageRating']])\n",
        "\n",
        "# Fill missing values in 'ContentRating' with the mode value\n",
        "mode_content_rating = movie_metadata['contentRating'].mode().iloc[0]\n",
        "movie_metadata['contentRating'].fillna(mode_content_rating, inplace=True)\n",
        "\n",
        "# Encode 'ContentRating' column using LabelEncoder\n",
        "le_content_rating = LabelEncoder()\n",
        "movie_metadata['contentRating'] = le_content_rating.fit_transform(movie_metadata['contentRating'])\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "movie_metadata['Genre'].fillna('', inplace=True)\n",
        "movie_metadata['actors'].fillna('', inplace=True)\n",
        "movie_metadata['Directors'].fillna('', inplace=True)\n",
        "movie_metadata['creators'].fillna('', inplace=True)\n",
        "movie_metadata['Keywords'].fillna('', inplace=True)\n",
        "movie_metadata['description'].fillna('', inplace=True)\n",
        "\n",
        "# Split comma-separated values into lists\n",
        "movie_metadata['Genre'] = movie_metadata['Genre'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['actors'] = movie_metadata['actors'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['Directors'] = movie_metadata['Directors'].apply(lambda x: str(x).split(', '))\n",
        "movie_metadata['creators'] = movie_metadata['creators'].apply(lambda x: str(x).split(', '))\n",
        "\n",
        "# Encode multi-value columns using MultiLabelBinarizer\n",
        "mlb_encoders = {}\n",
        "multi_label_columns = ['Genre', 'actors', 'Directors', 'creators']\n",
        "min_rows_with_1 = int(movie_metadata.shape[0] * 0.005)\n",
        "for col in multi_label_columns:\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    encoded_col = mlb.fit_transform(movie_metadata[col])\n",
        "    encoded_df = pd.DataFrame(encoded_col, columns=[f\"{col}_{c}\" for c in mlb.classes_], index=movie_metadata.index)\n",
        "    \n",
        "    # Filter columns with at least 100 rows containing a 1\n",
        "    cols_to_keep = encoded_df.columns[encoded_df.sum(axis=0) >= min_rows_with_1]\n",
        "    encoded_df = encoded_df[cols_to_keep]\n",
        "\n",
        "    movie_metadata = pd.concat([movie_metadata.drop(col, axis=1), encoded_df], axis=1)\n",
        "    mlb_encoders[col] = mlb\n",
        "\n",
        "# Encode text columns using TfidfVectorizer\n",
        "tfidf_encoders = {}\n",
        "max_features = 500\n",
        "text_columns = ['Keywords', 'description']\n",
        "for col in text_columns:\n",
        "    tfidf = TfidfVectorizer(max_features=max_features)\n",
        "    encoded_col = tfidf.fit_transform(movie_metadata[col].astype(str)).toarray()\n",
        "    encoded_df = pd.DataFrame(encoded_col, columns=[f\"{col}_{c}\" for c in tfidf.get_feature_names_out()], index=movie_metadata.index)\n",
        "    movie_metadata = pd.concat([movie_metadata.drop(col, axis=1), encoded_df], axis=1)\n",
        "    tfidf_encoders[col] = tfidf\n",
        "\n",
        "movie_metadata.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "movie_metadata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create a dense PyTorch tensor\n",
        "dense_tensor = torch.from_numpy(movie_metadata.to_numpy())\n",
        "\n",
        "# Calculate the number of non-zero elements in the tensor\n",
        "num_nonzero = torch.nonzero(dense_tensor).size(0)\n",
        "\n",
        "# Calculate the total number of elements in the tensor\n",
        "total_elements = dense_tensor.numel()\n",
        "\n",
        "# Calculate the sparsity ratio\n",
        "sparsity_ratio = 1.0 - (num_nonzero / total_elements)\n",
        "\n",
        "# Print the sparsity ratio\n",
        "print(\"Sparsity ratio: {:.2f}%\".format(sparsity_ratio * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3e5992741a799555bb326b04cdbb85fda14598ee",
        "id": "-UyJ3DKmw5jS"
      },
      "source": [
        "### Load user-data structure (1/4 to save memory + speed up compute) and preprocess to extract all rating to form a matrix. File structure is messy mix of json and csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "cf6473e25f7fd85d4896e1a87fd92b51f26fafa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "vuOM31sPw5jS",
        "outputId": "58cc4bd4-3197-4094-b1f3-ade46cb8d713"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "# Load single data-file\n",
        "df_raw = pd.read_csv(combined_data_1_path, header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
        "\n",
        "\n",
        "# Find empty rows to slice dataframe for each movie\n",
        "tmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\n",
        "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
        "\n",
        "# Shift the movie_indices by one to get start and endpoints of all movies\n",
        "shifted_movie_indices = deque(movie_indices)\n",
        "shifted_movie_indices.rotate(-1)\n",
        "\n",
        "\n",
        "# Gather all dataframes\n",
        "user_data = []\n",
        "\n",
        "# Iterate over all movies\n",
        "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
        "    \n",
        "    # Check if it is the last movie in the file\n",
        "    if df_id_1<df_id_2:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
        "    else:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
        "        \n",
        "    # Create movie_id column\n",
        "    tmp_df['Movie'] = movie_id\n",
        "    \n",
        "    # Append dataframe to list\n",
        "    user_data.append(tmp_df)\n",
        "\n",
        "# Combine all dataframes\n",
        "df = pd.concat(user_data)\n",
        "del user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
        "print('Shape User-Ratings:\\t{}'.format(df.shape))\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsAAgCpVit7"
      },
      "source": [
        "#### More formatting for user-data and only use X of the users (choose users with the most ratings) from (1/4) of the total data. Number subject to change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "sDxd-T17Vit7",
        "outputId": "057a4167-50f0-4fc5-e9db-5628371f0eb5"
      },
      "outputs": [],
      "source": [
        "unique_movies = df['Movie'].nunique()\n",
        "unique_users = df['User'].nunique()\n",
        "\n",
        "print(f'Number of unique users:\\t{unique_users}')\n",
        "print(f'Number of unique movies:\\t{unique_movies}')\n",
        "\n",
        "pct_movies = unique_movies\n",
        "pct_users = int(unique_users * 0.01)\n",
        "\n",
        "filter_movies = df['Movie'].value_counts().sort_values(ascending=False)[:pct_movies].index\n",
        "\n",
        "filter_users = df['User'].value_counts().sort_values(ascending=False)[:pct_users].index\n",
        "\n",
        "df_filtered = df[df[\"Movie\"].isin(filter_movies) & df[\"User\"].isin(filter_users)]\n",
        "del filter_movies, filter_users, df\n",
        "\n",
        "# rename the users and movies with new ids start from 0\n",
        "df_filtered['User'] = df_filtered['User'].astype(\"category\")\n",
        "df_filtered['Movie'] = df_filtered['Movie'].astype(\"category\")\n",
        "df_filtered['User'] = df_filtered['User'].cat.codes.values\n",
        "df_filtered['Movie'] = df_filtered['Movie'].cat.codes.values\n",
        "\n",
        "# make user the index and sort the index\n",
        "df_filtered.set_index('User', inplace=True)\n",
        "df_filtered.sort_index(inplace=True)\n",
        "\n",
        "print(f'Number users: {df_filtered.index.nunique()}')\n",
        "print(f'Number movies: {df_filtered[\"Movie\"].nunique()}')\n",
        "print(f'Shape: {df_filtered.shape}')\n",
        "df_filtered.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717sgV1vVit7"
      },
      "source": [
        "### Shuffle the filtered dataframe and split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5d286e3c457b425123827ccd8ab50eb62cf83530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "KuMjQjQxw5jU",
        "outputId": "21b438bf-0a99-4b7e-f976-d195a87d02bc"
      },
      "outputs": [],
      "source": [
        "# Shuffle DataFrame\n",
        "df_filtered = df_filtered.sample(frac=1).reset_index()\n",
        "\n",
        "percent_test = .2\n",
        "\n",
        "# create random seed\n",
        "import random\n",
        "seed = random.seed(42)\n",
        "\n",
        "\n",
        "# Split train and set set based on percentage\n",
        "df_train = df_filtered.sample(frac=1-percent_test, random_state=seed).reset_index(drop=True)\n",
        "df_test = df_filtered.drop(df_train.index).reset_index(drop=True)\n",
        "\n",
        "# split into X and y\n",
        "X_train = df_train.drop('Rating', axis=1)\n",
        "y_train = df_train['Rating']\n",
        "\n",
        "X_test = df_test.drop('Rating', axis=1)\n",
        "y_test = df_test['Rating']\n",
        "\n",
        "df_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHPD7RCVit7"
      },
      "source": [
        "## Baseline Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPbuqbaVit7"
      },
      "source": [
        "### Bellkor Algorithm \n",
        "Uses library from https://github.com/dandxy89/BellkorAlgorithm<br>\n",
        "Based on paper https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-m0R7NVit8"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nt358iwVit8",
        "outputId": "d1176348-3a42-4b5d-8c4c-4906ef0cb9f8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dandxy89/BellkorAlgorithm\n",
        "#!pip install -r {bellkor_requirements_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGsBJYP0Vit8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f'/content/{bellkor_import_path}')\n",
        "from Bellkor.Algorithm import BellkorAlgorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taCYCFAVit8"
      },
      "source": [
        "##### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RpEGpKVVit8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "df_train_timestamp = df_train.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_train_timestamp['timestamp'] = pd.to_datetime(df_train['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_train_timestamp[\"timestamp\"].min()\n",
        "end_time = df_train_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_train_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_train_timestamp[\"User\"].nunique()\n",
        "global_mean = df_train_timestamp[\"Rating\"].mean()\n",
        "average_df = df_train_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWVx8HO7Vit8"
      },
      "outputs": [],
      "source": [
        "calibrator = BellkorAlgorithm(n_items=movie_count, \n",
        "                              n_users=user_count, \n",
        "                              global_mean=global_mean,\n",
        "                              time_setting=dict(Start=adjusted_start_day, \n",
        "                                                End=adjusted_end_day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GIJ6bh3Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_train_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Ag8P0JVit8"
      },
      "outputs": [],
      "source": [
        "# TODO: increase sample size and #iterations for algorithm\n",
        "# TODO: Probably need to run a script to auto optimize parameters on this\n",
        "cost, error = calibrator.train(x=X, average_times=average_times, sample_size=100, iterations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATuW4yegVit8"
      },
      "source": [
        "##### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6_6s57pVit8"
      },
      "outputs": [],
      "source": [
        "# calc average_times and X for test data\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "df_test_timestamp = df_test.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_test_timestamp['timestamp'] = pd.to_datetime(df_filtered['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_test_timestamp[\"timestamp\"].min()\n",
        "end_time = df_test_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_test_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_test_timestamp[\"User\"].nunique()\n",
        "global_mean = df_test_timestamp[\"Rating\"].mean()\n",
        "average_df = df_test_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P25g-y7Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_test_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGMqBC0SVit9"
      },
      "outputs": [],
      "source": [
        "preds = calibrator.predict(x=X, average_times=average_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3vShyVZVit9"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def evaluate(preds, y_test):\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(preds.values, y_test.values))\n",
        "    print(\"RMSE: {}\".format(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjrm_oLAVit9"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame(data=preds, columns=[\"Index\", \"Prediction\"])\n",
        "predictions.head(n=10)\n",
        "\n",
        "# convert predictions to series\n",
        "predictions = pd.Series(data=predictions[\"Prediction\"].values, index=predictions[\"Index\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVS1tgC9Vit9",
        "outputId": "31c95c41-9232-4627-eda2-09ab3ae940b9"
      },
      "outputs": [],
      "source": [
        "evaluate(predictions, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQ0kyyMVit-"
      },
      "source": [
        "## Machine Learning Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filtered = df_filtered.drop('Date', axis=1)\n",
        "X_train = X_train.drop('Date', axis=1)\n",
        "X_test = X_test.drop('Date', axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A faster tensor data loader to batch load and speed up tabular data loading. Modified from source to support sparse tensors and loading from a dataset.\n",
        "\n",
        "Source: https://github.com/hcarlens/pytorch-tabular/blob/master/fast_tensor_data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FastDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a PyTorch Dataset that can be much faster than\n",
        "    DataLoader because DataLoader grabs individual indices of the dataset and\n",
        "    calls cat (slow).\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: Dataset, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastDatasetDataLoader.\n",
        "\n",
        "        :param dataset: Dataset object to load.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "\n",
        "        :returns: A FastDatasetDataLoader.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.dataset_len = len(dataset)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            self.permutation = torch.randperm(self.dataset_len)\n",
        "        else:\n",
        "            self.permutation = torch.arange(self.dataset_len)\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        indices = self.permutation[self.i:self.i+self.batch_size]\n",
        "        batch = tuple(torch.stack([self.dataset[idx][i] for idx in indices]) for i in range(len(self.dataset[0])))\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pi0qTFYVit-"
      },
      "source": [
        "### Matrix Factorization (Dot Product) w/ hidden layers\n",
        "Uses embeddings to represent users and movies. The dot product of user embeddings (n_users x e_dims) and movie embedding matrix (n_movies x e_dims) is a good approx of rating from user to movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCi-Z24pVit-"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, users, movie_ids, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.int)\n",
        "        self.movies = torch.tensor(movies, dtype=torch.int)\n",
        "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
        "\n",
        "class RecommenderModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, user_embedding_size, movie_embedding_size):\n",
        "        super(RecommenderModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, user_embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(n_movies, movie_embedding_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(user_embedding_size + movie_embedding_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        user_vector = self.user_embedding(user).squeeze(1)\n",
        "        movie_vector = self.movie_embedding(movie).squeeze(1)\n",
        "        # mul and sum are needed because dot only works with 1D tensors\n",
        "        x = torch.mul(user_vector, movie_vector).sum(1).unsqueeze(1)\n",
        "        cat = torch.cat([user_vector, movie_vector, x], dim=1)\n",
        "        dense = torch.relu(self.fc1(cat))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc2(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc3(dense))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = torch.relu(self.fc4(x))\n",
        "        dense = self.dropout(x)\n",
        "        y = self.fc5(x)\n",
        "        return y.flatten()\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for user, movie, rating in dataloader:\n",
        "        user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(user, movie)\n",
        "        loss = criterion(outputs, rating)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * user.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                loss = criterion(outputs, rating)\n",
        "                running_loss += loss.item() * user.size(0)\n",
        "        return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        ground_truth = []\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                predictions.extend(outputs.view(-1).cpu().numpy())\n",
        "                ground_truth.extend(rating.view(-1).cpu().numpy())\n",
        "        return np.sqrt(mean_squared_error(ground_truth, predictions))\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, model_name, restore_state=False):\n",
        "    if restore_state:\n",
        "        checkpoint = torch.load(f\"{google_save_path}/{model_name}-checkpoint.pt\")\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        model.load_state_dict(checkpoint['model_state_dict']).to(device)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    bad_epochs = 0\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        val_loss = validate(model, val_dataloader, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"{google_save_path}/{model_name}-best.pt\")\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= 5:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        # save the model checkpoint\n",
        "        if epoch % 5 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "            }, f\"{google_save_path}/{model_name}-checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcoMPtH_Vit_",
        "outputId": "9ab02567-1a58-4017-e697-4b62599d7ee3"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "n_users = df_filtered['User'].nunique()\n",
        "n_movies = df_filtered['Movie'].nunique()\n",
        "user_embedding_size = 20\n",
        "movie_embedding_size = 10\n",
        "batch_size = 1024\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RecommenderModel(n_users, n_movies, user_embedding_size, movie_embedding_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_dataset = MovieDataset(X_train['User'], X_train['Movie'], y_train)\n",
        "val_dataset = MovieDataset(X_test['User'], X_test['Movie'], y_test)\n",
        "train_dataloader = FastDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = FastDataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \"nn-dot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only used if we decide to do hyperparameter tuning\n",
        "# The user dataset would need to be split into three parts\n",
        "# Evaluation\n",
        "test_dataset = MovieDataset(X_test['User'], X_test['Movie'], y_test)\n",
        "test_dataloader = FastDataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(f\"{google_save_path}/nn-dot.pt\"))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "rmse = evaluate(model, test_dataloader, device)\n",
        "print(f\"Test RMSE: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: REMOVE OR USE\n",
        "\n",
        "# Making predictions\n",
        "# def predict(model, user, movie, device):\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         user_tensor = torch.tensor([user], dtype=torch.long, device=device).unsqueeze(0)\n",
        "#         movie_tensor = torch.tensor([movie], dtype=torch.long, device=device).unsqueeze(0)\n",
        "#         output = model(user_tensor, movie_tensor)\n",
        "#         return output.item()\n",
        "\n",
        "# user_id = 1\n",
        "# movie_id = 100\n",
        "\n",
        "#prediction = predict(model, user_id, movie_id, device)\n",
        "#print(f\"Predicted rating for user {user_id} and movie {movie_id}: {prediction}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DpXmsGjNViuA"
      },
      "source": [
        "### The Deep Hybrid System with Metadata\n",
        "Uses movie metadata to improve recommendations. Metadata is combined with embeddings of user-id and movie-id."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDHjXoVViuB"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "class HybridMovieDataset(Dataset):\n",
        "    def __init__(self, users, movie_ids, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.int)\n",
        "        self.movie_ids = torch.tensor(movies, dtype=torch.int)\n",
        "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.movie_ids[idx], self.ratings[idx]\n",
        "\n",
        "class HybridRecommenderModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, user_embedding_size, movie_embedding_size):\n",
        "        super(RecommenderModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, user_embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(n_movies, movie_embedding_size)\n",
        "        self.movie_metadata = nn.Linear(metadata_size, movie_embedding_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = torch.relu(nn.Linear(user_embedding_size + movie_embedding_size, 256))\n",
        "        self.fc2 = torch.relu(nn.Linear(256, 128))\n",
        "        self.fc3 = torch.relu(nn.Linear(128, 64))\n",
        "        self.fc4 = torch.relu(nn.Linear(64, 32))\n",
        "        self.m1 = torch.relu(nn.Linear(128, 32))\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        user_vector = self.user_embedding(user).squeeze(1)\n",
        "        movie_vector = self.movie_embedding(movie).squeeze(1)\n",
        "        metadata_vector = self.movie_metadata(movie_metadata)\n",
        "\n",
        "        metadata_vector = torch.relu(self.fc1(metadata_vector))\n",
        "        metadata_vector = self.dropout(metadata_vector)\n",
        "        metadata_vector = torch.relu(self.fc2(metadata_vector))\n",
        "        metadata_vector = self.dropout(metadata_vector)\n",
        "        metadata_vector = torch.relu(self.m1(metadata_vector))\n",
        "        metadata_vector = self.dropout(metadata_vector)\n",
        "        metadata_vector = self.fc5(metadata_vector)\n",
        "\n",
        "        # mul and sum are needed because dot only works with 1D tensors\n",
        "        x = torch.mul(user_vector, movie_vector).sum(1).unsqueeze(1)\n",
        "        cat = torch.cat([user_vector, movie_vector, metadata_vector, x], dim=1)\n",
        "        dense = torch.relu(self.fc1(cat))\n",
        "        dense = self.dropout(dense)\n",
        "        dense = self.fc2(dense)\n",
        "        dense = self.dropout(dense)\n",
        "        dense = self.fc3(dense)\n",
        "        dense = self.dropout(dense)\n",
        "        dense = self.fc4(x)\n",
        "        dense = self.dropout(x)\n",
        "        y = self.fc5(x)\n",
        "        return y.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "n_users = df_filtered['User'].nunique()\n",
        "n_movies = df_filtered['Movie'].nunique()\n",
        "user_embedding_size = 20\n",
        "movie_embedding_size = 10\n",
        "batch_size = 1024\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = HybridRecommenderModel(n_users, n_movies, user_embedding_size, movie_embedding_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_dataset = HybridMovieDataset(X_train['User'], X_train['Movie'], movie_metadata, y_train)\n",
        "val_dataset = HybridMovieDataset(X_test['User'], X_test['Movie'], movie_metadata, y_test)\n",
        "train_dataloader = FastDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = FastDataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "training_loop(n_epochs, optimizer, model, criterion, train_dataloader, val_dataloader, device, \"nn-hybrid\")\n",
        "\n",
        "torch.save(model.state_dict(), f\"{google_save_path}/nn-metadata.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
