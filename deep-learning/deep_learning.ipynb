{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjMrREk4Vit0"
      },
      "source": [
        "### Colab Setup (Don't run this cell if you're not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYQswrKJVit3",
        "outputId": "46eb3f18-e310-4419-d262-665a7b4cf1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mn3ndin4Vit4"
      },
      "outputs": [],
      "source": [
        "movie_titles_path = '/content/drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '/content/drive/MyDrive/CSC 422/CSC422 Class Project/codes/checkpoints/merge4.csv'\n",
        "combined_data_1_path = '/content/drive/MyDrive/CSC 422/CSC422 Class Project/codes/prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-mXf6aVit4"
      },
      "source": [
        "### Non-Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pOVKd0kAVit4"
      },
      "outputs": [],
      "source": [
        "movie_titles_path = '../prize_dataset/movie_titles.csv'\n",
        "movie_metadata_path = '../IMDB_data/merge4.csv'\n",
        "combined_data_1_path = '../prize_dataset/combined_data_1.txt'\n",
        "bellkor_requirements_path = './BellkorAlgorithm/requirements.txt'\n",
        "bellkor_import_path = 'BellkorAlgorithm/Bellkor'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJYtb7CKVit5",
        "outputId": "e249f71c-e6d4-4726-ee35-74b534970fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 11 17:51:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# See the GPU specs\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "owNUR4Jzw5jQ"
      },
      "outputs": [],
      "source": [
        "# To store the data\n",
        "#import pandas as pd\n",
        "\n",
        "# To do linear algebra\n",
        "#import numpy as np\n",
        "\n",
        "# To create plots\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# To create interactive plots\n",
        "#import nbformat\n",
        "#from plotly.offline import init_notebook_mode, plot, iplot\n",
        "#import plotly.graph_objs as go\n",
        "#init_notebook_mode(connected=True)\n",
        "\n",
        "# To compute similarities between vectors\n",
        "#from sklearn.metrics import mean_squared_error\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "##from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# To create sparse matrices\n",
        "#from scipy.sparse import coo_matrix\n",
        "\n",
        "# To stack sparse matrices\n",
        "#from scipy.sparse import vstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6kd2-B2AVit5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEJu3DbPVit5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81ecc38ecc2650b81c042a385599a3af31b4e1e6",
        "id": "NU1QuoEWw5jR"
      },
      "source": [
        "### Load Movie Tiles w/o metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "a698fdfcf9ac8ef2193c3b40503b92283c5bec8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "-lSOFdGKw5jS",
        "outputId": "5ff913d9-283c-4828-dd82-371410f924fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape Movie-Titles:\t(17770, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Year                                     Name\n",
              "Id                                                    \n",
              "17619  1947.0                          Out of the Past\n",
              "15245  1996.0  Hetty Wainthropp Investigates: Series 1\n",
              "3031   1996.0                         Pompatus of Love\n",
              "4492   2004.0                               Club Dread\n",
              "3351   1987.0             Gospel According to Al Green"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f6137c7-2fe9-430a-a19c-7ee55ca1ba62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17619</th>\n",
              "      <td>1947.0</td>\n",
              "      <td>Out of the Past</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15245</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>Hetty Wainthropp Investigates: Series 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3031</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>Pompatus of Love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4492</th>\n",
              "      <td>2004.0</td>\n",
              "      <td>Club Dread</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3351</th>\n",
              "      <td>1987.0</td>\n",
              "      <td>Gospel According to Al Green</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f6137c7-2fe9-430a-a19c-7ee55ca1ba62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f6137c7-2fe9-430a-a19c-7ee55ca1ba62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f6137c7-2fe9-430a-a19c-7ee55ca1ba62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from io import StringIO\n",
        "import re\n",
        "\n",
        "for_pd = StringIO()\n",
        "with open(movie_titles_path, encoding = 'ISO-8859-1') as movie_titles:\n",
        "    for line in movie_titles:\n",
        "        new_line = re.sub(r',', '|', line.rstrip(), count=2)\n",
        "        print (new_line, file=for_pd)\n",
        "\n",
        "for_pd.seek(0)\n",
        "\n",
        "movie_titles = pd.read_csv(for_pd, sep='|', header=None, names=['Id', 'Year', 'Name']).set_index('Id')\n",
        "del for_pd\n",
        "\n",
        "print('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\n",
        "movie_titles.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c0612cd9da11d6c8a984db24f2befec720f8cdbc",
        "id": "DAKkIpSaw5jS"
      },
      "source": [
        "### Load Movie Titles w/ metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "HOzmfXNFw5jS",
        "outputId": "c940c357-6104-48ad-fe99-ec07334887b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                               description  \\\n",
              "Name                                                                                         \n",
              "Legend of Crystania: The Motion Picture  Dreams begin to haunt Lady Sheru -- dreams of ...   \n",
              "America's Most Haunted Inns              Have you ever seen a ghost? After seeing this ...   \n",
              "Sacred Silence                           A young priest crusades against organized crim...   \n",
              "Moonlight and Valentino                  A young widow still grieving over the death of...   \n",
              "Bullet                                   Paroled after 8 years in prison, Bullet's pick...   \n",
              "\n",
              "                                         NumRating  \n",
              "Name                                                \n",
              "Legend of Crystania: The Motion Picture      252.0  \n",
              "America's Most Haunted Inns                   40.0  \n",
              "Sacred Silence                               407.0  \n",
              "Moonlight and Valentino                     3815.0  \n",
              "Bullet                                      8617.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3d61137-39cf-40be-aa31-5668fe49a586\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>NumRating</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Legend of Crystania: The Motion Picture</th>\n",
              "      <td>Dreams begin to haunt Lady Sheru -- dreams of ...</td>\n",
              "      <td>252.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>America's Most Haunted Inns</th>\n",
              "      <td>Have you ever seen a ghost? After seeing this ...</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sacred Silence</th>\n",
              "      <td>A young priest crusades against organized crim...</td>\n",
              "      <td>407.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moonlight and Valentino</th>\n",
              "      <td>A young widow still grieving over the death of...</td>\n",
              "      <td>3815.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bullet</th>\n",
              "      <td>Paroled after 8 years in prison, Bullet's pick...</td>\n",
              "      <td>8617.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3d61137-39cf-40be-aa31-5668fe49a586')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3d61137-39cf-40be-aa31-5668fe49a586 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3d61137-39cf-40be-aa31-5668fe49a586');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "movie_metadata_raw = pd.read_csv(movie_metadata_path)\n",
        "\n",
        "movie_metadata = movie_metadata_raw[movie_metadata_raw['imdbID'].notnull()][['Name', 'description', 'NumRating']].set_index('Name').dropna()\n",
        "del movie_metadata_raw\n",
        "movie_metadata.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3e5992741a799555bb326b04cdbb85fda14598ee",
        "id": "-UyJ3DKmw5jS"
      },
      "source": [
        "### Load user-data structure (1/4 to save memory + speed up compute) and preprocess to extract all rating to form a matrix. File structure is messy mix of json and csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "cf6473e25f7fd85d4896e1a87fd92b51f26fafa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "vuOM31sPw5jS",
        "outputId": "58cc4bd4-3197-4094-b1f3-ade46cb8d713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape User-Ratings:\t(24053764, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             User  Rating        Date  Movie\n",
              "20836965  1692256     4.0  2003-11-03   3925\n",
              "22314237  1130409     5.0  2004-11-10   4227\n",
              "20785876  2473613     1.0  2005-06-23   3925\n",
              "23622086   743002     4.0  2004-11-30   4420\n",
              "10908647  2230283     5.0  2004-10-26   2122"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e92c522-6b19-4046-b49b-41be10fe552b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20836965</th>\n",
              "      <td>1692256</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2003-11-03</td>\n",
              "      <td>3925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22314237</th>\n",
              "      <td>1130409</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2004-11-10</td>\n",
              "      <td>4227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20785876</th>\n",
              "      <td>2473613</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2005-06-23</td>\n",
              "      <td>3925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23622086</th>\n",
              "      <td>743002</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2004-11-30</td>\n",
              "      <td>4420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10908647</th>\n",
              "      <td>2230283</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2004-10-26</td>\n",
              "      <td>2122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e92c522-6b19-4046-b49b-41be10fe552b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e92c522-6b19-4046-b49b-41be10fe552b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e92c522-6b19-4046-b49b-41be10fe552b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from collections import deque\n",
        "\n",
        "# Load single data-file\n",
        "df_raw = pd.read_csv(combined_data_1_path, header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n",
        "\n",
        "\n",
        "# Find empty rows to slice dataframe for each movie\n",
        "tmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\n",
        "movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
        "\n",
        "# Shift the movie_indices by one to get start and endpoints of all movies\n",
        "shifted_movie_indices = deque(movie_indices)\n",
        "shifted_movie_indices.rotate(-1)\n",
        "\n",
        "\n",
        "# Gather all dataframes\n",
        "user_data = []\n",
        "\n",
        "# Iterate over all movies\n",
        "for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
        "    \n",
        "    # Check if it is the last movie in the file\n",
        "    if df_id_1<df_id_2:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
        "    else:\n",
        "        tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
        "        \n",
        "    # Create movie_id column\n",
        "    tmp_df['Movie'] = movie_id\n",
        "    \n",
        "    # Append dataframe to list\n",
        "    user_data.append(tmp_df)\n",
        "\n",
        "# Combine all dataframes\n",
        "df = pd.concat(user_data)\n",
        "del user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\n",
        "print('Shape User-Ratings:\\t{}'.format(df.shape))\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZsAAgCpVit7"
      },
      "source": [
        "#### More formatting for user-data and only use X of the users (choose users with the most ratings) from (1/4) of the total data. Number subject to change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "sDxd-T17Vit7",
        "outputId": "057a4167-50f0-4fc5-e9db-5628371f0eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users:\t470758\n",
            "Number of unique movies:\t4499\n",
            "Number users: 141227\n",
            "Number movies: 4499\n",
            "Shape: (18338551, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Rating        Date  Movie\n",
              "User                           \n",
              "0        4.0  2004-04-29    808\n",
              "0        2.0  2005-10-04   4091\n",
              "0        3.0  2005-05-03   2455\n",
              "0        3.0  2004-08-03    110\n",
              "0        4.0  2004-11-09   1743"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9cb03e62-7203-4570-bdb3-2c00f3f9cfba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>User</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2004-04-29</td>\n",
              "      <td>808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2005-10-04</td>\n",
              "      <td>4091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2005-05-03</td>\n",
              "      <td>2455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2004-08-03</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2004-11-09</td>\n",
              "      <td>1743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cb03e62-7203-4570-bdb3-2c00f3f9cfba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9cb03e62-7203-4570-bdb3-2c00f3f9cfba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9cb03e62-7203-4570-bdb3-2c00f3f9cfba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "unique_movies = df['Movie'].nunique()\n",
        "unique_users = df['User'].nunique()\n",
        "\n",
        "print(f'Number of unique users:\\t{unique_users}')\n",
        "print(f'Number of unique movies:\\t{unique_movies}')\n",
        "\n",
        "pct_movies = unique_movies\n",
        "pct_users = int(unique_users * 0.3)\n",
        "\n",
        "filter_movies = df['Movie'].value_counts().sort_values(ascending=False)[:pct_movies].index\n",
        "\n",
        "filter_users = df['User'].value_counts().sort_values(ascending=False)[:pct_users].index\n",
        "\n",
        "df_filtered = df[df[\"Movie\"].isin(filter_movies) & df[\"User\"].isin(filter_users)]\n",
        "del filter_movies, filter_users, df\n",
        "\n",
        "# rename the users and movies with new ids start from 0\n",
        "df_filtered['User'] = df_filtered['User'].astype(\"category\")\n",
        "df_filtered['Movie'] = df_filtered['Movie'].astype(\"category\")\n",
        "df_filtered['User'] = df_filtered['User'].cat.codes.values\n",
        "df_filtered['Movie'] = df_filtered['Movie'].cat.codes.values\n",
        "\n",
        "# make user the index and sort the index\n",
        "df_filtered.set_index('User', inplace=True)\n",
        "df_filtered.sort_index(inplace=True)\n",
        "\n",
        "print(f'Number users: {df_filtered.index.nunique()}')\n",
        "print(f'Number movies: {df_filtered[\"Movie\"].nunique()}')\n",
        "print(f'Shape: {df_filtered.shape}')\n",
        "df_filtered.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717sgV1vVit7"
      },
      "source": [
        "### Shuffle the filtered dataframe and split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5d286e3c457b425123827ccd8ab50eb62cf83530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "KuMjQjQxw5jU",
        "outputId": "21b438bf-0a99-4b7e-f976-d195a87d02bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     User  Rating        Date  Movie\n",
              "0   33535     4.0  2003-05-30    758\n",
              "1   47919     5.0  2005-07-16   3863\n",
              "2   82542     4.0  2004-11-01   4392\n",
              "3   98082     5.0  2005-07-21    850\n",
              "4  100949     3.0  2004-05-27   1598\n",
              "5   68264     5.0  2005-09-27   3017\n",
              "6   23654     4.0  2005-06-01   2432\n",
              "7  127163     3.0  2004-06-21   4261\n",
              "8    8324     1.0  2004-08-06   3611\n",
              "9    7247     2.0  2005-08-26   2557"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61b88fd3-978c-48bf-9663-dd2d0f38f0c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>Movie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33535</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2003-05-30</td>\n",
              "      <td>758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>47919</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2005-07-16</td>\n",
              "      <td>3863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82542</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2004-11-01</td>\n",
              "      <td>4392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98082</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2005-07-21</td>\n",
              "      <td>850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100949</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2004-05-27</td>\n",
              "      <td>1598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>68264</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2005-09-27</td>\n",
              "      <td>3017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>23654</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2005-06-01</td>\n",
              "      <td>2432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>127163</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2004-06-21</td>\n",
              "      <td>4261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8324</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2004-08-06</td>\n",
              "      <td>3611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7247</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2005-08-26</td>\n",
              "      <td>2557</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61b88fd3-978c-48bf-9663-dd2d0f38f0c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61b88fd3-978c-48bf-9663-dd2d0f38f0c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61b88fd3-978c-48bf-9663-dd2d0f38f0c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Shuffle DataFrame\n",
        "df_filtered = df_filtered.sample(frac=1).reset_index()\n",
        "\n",
        "percent_test = .2\n",
        "\n",
        "# create random seed\n",
        "import random\n",
        "seed = random.seed(42)\n",
        "\n",
        "\n",
        "# Split train and set set based on percentage\n",
        "df_train = df_filtered.sample(frac=1-percent_test, random_state=seed).reset_index(drop=True)\n",
        "df_test = df_filtered.drop(df_train.index).reset_index(drop=True)\n",
        "\n",
        "# split into X and y\n",
        "X_train = df_train.drop('Rating', axis=1)\n",
        "y_train = df_train['Rating']\n",
        "\n",
        "X_test = df_test.drop('Rating', axis=1)\n",
        "y_test = df_test['Rating']\n",
        "\n",
        "df_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHPD7RCVit7"
      },
      "source": [
        "## Baseline Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPbuqbaVit7"
      },
      "source": [
        "### Bellkor Algorithm \n",
        "Uses library from https://github.com/dandxy89/BellkorAlgorithm<br>\n",
        "Based on paper https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-m0R7NVit8"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nt358iwVit8",
        "outputId": "d1176348-3a42-4b5d-8c4c-4906ef0cb9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'BellkorAlgorithm' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython==8.10.0\n",
            "  Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
            "Collecting notebook==6.4.12\n",
            "  Using cached notebook-6.4.12-py3-none-any.whl (9.9 MB)\n",
            "Collecting numpy==1.22.0\n",
            "  Using cached numpy-1.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Collecting pandas==1.0.3\n",
            "  Using cached pandas-1.0.3.tar.gz (5.0 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dandxy89/BellkorAlgorithm\n",
        "#!pip install -r {bellkor_requirements_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGsBJYP0Vit8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f'/content/{bellkor_import_path}')\n",
        "from Bellkor.Algorithm import BellkorAlgorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taCYCFAVit8"
      },
      "source": [
        "##### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_RpEGpKVVit8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "df_train_timestamp = df_train.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_train_timestamp['timestamp'] = pd.to_datetime(df_train['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_train_timestamp[\"timestamp\"].min()\n",
        "end_time = df_train_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_train_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_train_timestamp[\"User\"].nunique()\n",
        "global_mean = df_train_timestamp[\"Rating\"].mean()\n",
        "average_df = df_train_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kWVx8HO7Vit8"
      },
      "outputs": [],
      "source": [
        "calibrator = BellkorAlgorithm(n_items=movie_count, \n",
        "                              n_users=user_count, \n",
        "                              global_mean=global_mean,\n",
        "                              time_setting=dict(Start=adjusted_start_day, \n",
        "                                                End=adjusted_end_day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6GIJ6bh3Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_train_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y0Ag8P0JVit8"
      },
      "outputs": [],
      "source": [
        "# TODO: increase sample size and #iterations for algorithm\n",
        "# TODO: Probably need to run a script to auto optimize parameters on this\n",
        "cost, error = calibrator.train(x=X, average_times=average_times, sample_size=100, iterations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATuW4yegVit8"
      },
      "source": [
        "##### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T6_6s57pVit8"
      },
      "outputs": [],
      "source": [
        "# calc average_times and X for test data\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "df_test_timestamp = df_test.copy().rename(columns={'Date': 'timestamp'})\n",
        "df_test_timestamp['timestamp'] = pd.to_datetime(df_filtered['Date']).apply(lambda x: datetime.datetime.timestamp(x)).astype(int)\n",
        "start_time = df_test_timestamp[\"timestamp\"].min()\n",
        "end_time = df_test_timestamp[\"timestamp\"].max()\n",
        "\n",
        "\n",
        "adjusted_start_day = int(time.mktime(datetime.datetime.fromtimestamp(start_time).date().timetuple()))\n",
        "adjusted_end_day = int(time.mktime(datetime.datetime.fromtimestamp(end_time).date().timetuple())) + 86400\n",
        "movie_count = df_test_timestamp[\"Movie\"].nunique()\n",
        "user_count = df_test_timestamp[\"User\"].nunique()\n",
        "global_mean = df_test_timestamp[\"Rating\"].mean()\n",
        "average_df = df_test_timestamp.groupby(\"User\")[\"timestamp\"].mean().reset_index()\n",
        "average_times = pd.Series(average_df.timestamp.values, index=average_df.User).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2P25g-y7Vit8"
      },
      "outputs": [],
      "source": [
        "#rename the columns\n",
        "X = df_test_timestamp.rename(columns={'User': 'UserId', 'Movie': 'MovieId', 'Rating': 'rating'}, inplace=False)\n",
        "indices = X.index.values\n",
        "X = X.loc[:, [\"timestamp\", \"UserId\", \"MovieId\", \"rating\"]].to_numpy()\n",
        "\n",
        "# add index to the front of x\n",
        "X = np.insert(X, 0, indices, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YGMqBC0SVit9"
      },
      "outputs": [],
      "source": [
        "preds = calibrator.predict(x=X, average_times=average_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W3vShyVZVit9"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def evaluate(preds, y_test):\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(preds.values, y_test.values))\n",
        "    print(\"RMSE: {}\".format(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fjrm_oLAVit9"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame(data=preds, columns=[\"Index\", \"Prediction\"])\n",
        "predictions.head(n=10)\n",
        "\n",
        "# convert predictions to series\n",
        "predictions = pd.Series(data=predictions[\"Prediction\"].values, index=predictions[\"Index\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVS1tgC9Vit9",
        "outputId": "31c95c41-9232-4627-eda2-09ab3ae940b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.0279828962725628\n"
          ]
        }
      ],
      "source": [
        "evaluate(predictions, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgxqzUtxVit9"
      },
      "source": [
        "### Weighted Mean Rating (!!!Needs conversion for RMSE testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGZ8W-ZDVit9"
      },
      "source": [
        "##### Setup\n",
        "Create sparse matrix. Each row represents a user and its ratings and the columns are the movies. We're interested in finding the empty values (unrated movies for that user)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kJWk_sRVit9"
      },
      "outputs": [],
      "source": [
        "# Create a user-movie matrix with empty values\n",
        "df_p = df_train.pivot_table(index='User', columns='Movie', values='Rating')\n",
        "print('Shape User-Movie-Matrix:\\t{}'.format(df_p.shape))\n",
        "df_p.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEzxkkOeVit-"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFT-LyvoVit-"
      },
      "outputs": [],
      "source": [
        "# Number of minimum votes to be considered\n",
        "m = 1000\n",
        "\n",
        "# Mean rating for all movies\n",
        "C = df_p.stack().mean()\n",
        "\n",
        "# Mean rating for all movies separatly\n",
        "R = df_p.mean(axis=0).values\n",
        "\n",
        "# Rating count for all movies separatly\n",
        "v = df_p.count().values\n",
        "\n",
        "\n",
        "# Weighted formula to compute the weighted rating\n",
        "weighted_score = (v/ (v+m) *R) + (m/ (v+m) *C)\n",
        "# Sort ids to ranking\n",
        "weighted_ranking = np.argsort(weighted_score)[::-1]\n",
        "# Sort scores to ranking\n",
        "weighted_score = np.sort(weighted_score)[::-1]\n",
        "# Get movie ids\n",
        "weighted_movie_ids = df_p.columns[weighted_ranking]\n",
        "\n",
        "\n",
        "# Join labels and predictions\n",
        "df_prediction = df_test.set_index('Movie').join(pd.DataFrame(weighted_score, index=weighted_movie_ids, columns=['Prediction']))[['Rating', 'Prediction']]\n",
        "y_true = df_prediction['Rating']\n",
        "y_pred = df_prediction['Prediction']\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-srt2JsNVit-"
      },
      "source": [
        "### Cosine User-User Similarity (!!!Needs conversion for RMSE testing) (prob not use) (Don't use midterm paper)\n",
        "Interpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.<br>\n",
        "Since there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values.**<br>\n",
        "Afterwards the **ratings of all similar users will be weighted with their similarity score and the mean will be computed.** Filtering for the unrated movies of a user reveals the best recommendations.<br>\n",
        "You can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O_PWPZxVit-"
      },
      "outputs": [],
      "source": [
        "#from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "##from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# User index for recommendation\n",
        "user_index = 0\n",
        "\n",
        "# Number of similar users for recommendation\n",
        "n_recommendation = 100\n",
        "\n",
        "# Plot top n recommendations\n",
        "n_plot = 10\n",
        "\n",
        "\n",
        "# Fill in missing values\n",
        "df_p_imputed = df_p.T.fillna(df_p.mean(axis=1)).T\n",
        "\n",
        "# Compute similarity between all users\n",
        "similarity = cosine_similarity(df_p_imputed.values)\n",
        "\n",
        "# Remove self-similarity from similarity-matrix\n",
        "similarity -= np.eye(similarity.shape[0])\n",
        "\n",
        "\n",
        "# Sort similar users by index\n",
        "similar_user_index = np.argsort(similarity[user_index])[::-1]\n",
        "# Sort similar users by score\n",
        "similar_user_score = np.sort(similarity[user_index])[::-1]\n",
        "\n",
        "\n",
        "# Get unrated movies\n",
        "unrated_movies = df_p.iloc[user_index][df_p.iloc[user_index].isna()].index\n",
        "\n",
        "# Weight ratings of the top n most similar users with their rating and compute the mean for each movie\n",
        "mean_movie_recommendations = (df_p_imputed.iloc[similar_user_index[:n_recommendation]].T * similar_user_score[:n_recommendation]).T.mean(axis=0)\n",
        "\n",
        "# Filter for unrated movies and sort results\n",
        "best_movie_recommendations = mean_movie_recommendations[unrated_movies].sort_values(ascending=False).to_frame().join(movie_titles)\n",
        "\n",
        "\n",
        "# Create user-id mapping\n",
        "user_id_mapping = {id:i for i, id in enumerate(df_p_imputed.index)}\n",
        "\n",
        "prediction = []\n",
        "# Iterate over all testset items\n",
        "for user_id in df_test['User'].unique():\n",
        "    \n",
        "    # Sort similar users by index\n",
        "    similar_user_index = np.argsort(similarity[user_id_mapping[user_id]])[::-1]\n",
        "    # Sort similar users by score\n",
        "    similar_user_score = np.sort(similarity[user_id_mapping[user_id]])[::-1]\n",
        "    \n",
        "    for movie_id in df_test[df_test['User']==user_id]['Movie'].values:\n",
        "\n",
        "        # Compute predicted score\n",
        "        score = (df_p_imputed.iloc[similar_user_index[:n_recommendation]][movie_id] * similar_user_score[:n_recommendation]).values.sum() / similar_user_score[:n_recommendation].sum()\n",
        "        prediction.append([user_id, movie_id, score])\n",
        "        \n",
        "\n",
        "# Create prediction DataFrame\n",
        "df_pred = pd.DataFrame(prediction, columns=['User', 'Movie', 'Prediction']).set_index(['User', 'Movie'])\n",
        "df_pred = df_test.set_index(['User', 'Movie']).join(df_pred)\n",
        "\n",
        "\n",
        "# Get labels and predictions\n",
        "y_true = df_pred['Rating'].values\n",
        "y_pred = df_pred['Prediction'].values\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n",
        "\n",
        "\n",
        "# Create trace\n",
        "trace = go.Bar(x = best_movie_recommendations.iloc[:n_plot, 0],\n",
        "               text = best_movie_recommendations['Name'],\n",
        "               textposition = 'inside',\n",
        "               textfont = dict(color = '#000000'),\n",
        "               orientation = 'h',\n",
        "               y = list(range(1, n_plot+1)),\n",
        "               marker = dict(color = '#db0000'))\n",
        "# Create layout\n",
        "layout = dict(title = 'Ranking Of Top {} Recommended Movies For A User Based On Similarity: {:.4f} RMSE'.format(n_plot, rmse),\n",
        "              xaxis = dict(title = 'Recommendation-Rating',\n",
        "                           range = (4.1, 4.5)),\n",
        "              yaxis = dict(title = 'Movie'))\n",
        "# Create plot\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQ0kyyMVit-"
      },
      "source": [
        "## Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pi0qTFYVit-"
      },
      "source": [
        "### Matrix Factorization (Dot Product) w/ hidden layers\n",
        "Uses embeddings to represent users and movies. The dot product of user embeddings (n_users x e_dims) and movie embedding matrix (n_movies x e_dims) is a good approx of rating from user to movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCi-Z24pVit-"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJaWT4VYVit-",
        "outputId": "105fe6c1-82aa-4efd-8a10-6b0af9942ee2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_filtered' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_filtered \u001b[39m=\u001b[39m df_filtered\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_filtered' is not defined"
          ]
        }
      ],
      "source": [
        "df_filtered = df_filtered.drop('Date', axis=1)\n",
        "X_train = X_train.drop('Date', axis=1)\n",
        "X_test = X_test.drop('Date', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcoMPtH_Vit_",
        "outputId": "9ab02567-1a58-4017-e697-4b62599d7ee3"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 9.94 GiB already allocated; 57.44 MiB free; 9.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m e_dimension \u001b[39m=\u001b[39m \u001b[39m6000\u001b[39m\n\u001b[1;32m     64\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m model \u001b[39m=\u001b[39m RecommenderModel(n_users, n_movies, e_dimension)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     66\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     67\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.003\u001b[39m)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.05 GiB (GPU 0; 10.76 GiB total capacity; 9.94 GiB already allocated; 57.44 MiB free; 9.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# TODO: Figure out if we the code for correct mapping or can just use the original data\n",
        "\n",
        "# # Create user- & movie-id mapping\n",
        "# user_id_mapping = {id:i for i, id in enumerate(df_filtered['User'].unique())}\n",
        "# movie_id_mapping = {id:i for i, id in enumerate(df_filtered['Movie'].unique())}\n",
        "\n",
        "# # Create correctly mapped train- & testset\n",
        "# train_user_data = df_train['User'].map(user_id_mapping)\n",
        "# train_movie_data = df_train['Movie'].map(movie_id_mapping)\n",
        "\n",
        "# test_user_data = df_test['User'].map(user_id_mapping)\n",
        "# test_movie_data = df_test['Movie'].map(movie_id_mapping)\n",
        "\n",
        "# # Get input variable-sizes\n",
        "# users = len(user_id_mapping)\n",
        "# movies = len(movie_id_mapping)\n",
        "# embedding_size = 10\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[0][idx], self.x[1][idx], self.y[idx]\n",
        "\n",
        "class RecommenderModel(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, e_dimension):\n",
        "        super(RecommenderModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(n_users, e_dimension)\n",
        "        self.movie_embedding = nn.Embedding(n_movies, e_dimension)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(2 * e_dimension + 1, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        u = self.user_embedding(user).squeeze(1)\n",
        "        m = self.movie_embedding(movie).squeeze(1)\n",
        "        x = torch.mul(u, m).sum(1).unsqueeze(1)\n",
        "        x = torch.cat([u, m, x], dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x.flatten()\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for user, movie, rating in dataloader:\n",
        "        user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(user, movie)\n",
        "        loss = criterion(outputs, rating)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * user.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                loss = criterion(outputs, rating)\n",
        "                running_loss += loss.item() * user.size(0)\n",
        "        return running_loss / len(dataloader.dataset)\n",
        "\n",
        "e_dimension = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RecommenderModel(n_users, n_movies, e_dimension).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_dataset = MovieDataset(torch.tensor(X_train.values, dtype=torch.long), torch.tensor(y_train.values, dtype=torch.float))\n",
        "val_dataset = MovieDataset(torch.tensor(X_test.values, dtype=torch.long), torch.tensor(y_test.values, dtype=torch.float))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "n_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "bad_epochs = 0\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    val_loss = validate(model, val_dataloader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"Model_1.pt\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= 5:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        ground_truth = []\n",
        "        with torch.no_grad():\n",
        "            for user, movie, rating in dataloader:\n",
        "                user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
        "                outputs = model(user, movie)\n",
        "                predictions.extend(outputs.view(-1).cpu().numpy())\n",
        "                ground_truth.extend(rating.view(-1).cpu().numpy())\n",
        "        return np.sqrt(mean_squared_error(ground_truth, predictions))\n",
        "\n",
        "test_dataset = MovieDataset(torch.tensor(X_test.values, dtype=torch.long), torch.tensor(y_test.values, dtype=torch.float))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"model_mf_dot_product_w_hidden.pt\"))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "rmse = evaluate(model, test_dataloader, device)\n",
        "print(f\"Test RMSE: {rmse}\")\n",
        "\n",
        "# Making predictions\n",
        "def predict(model, user, movie, device):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        user_tensor = torch.tensor([user], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        movie_tensor = torch.tensor([movie], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        output = model(user_tensor, movie_tensor)\n",
        "        return output.item()\n",
        "\n",
        "user_id = 1\n",
        "movie_id = 100\n",
        "\n",
        "#prediction = predict(model, user_id, movie_id, device)\n",
        "#print(f\"Predicted rating for user {user_id} and movie {movie_id}: {prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fea679ee86b2eda4e76f26343dfb2b32ec86ff7f",
        "id": "uBm7sgyDw5jV"
      },
      "source": [
        "#### Matrix Factorization + Gradient Descent\n",
        "Reduces dimensionality to represent data in dense form using embeddings. Then calculates dot product of user and movie embeddings to get rating prediction. Uses gradient descent to optimize embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmq3IU36Vit_"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8edOXuLVit_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Create user- & movie-id mapping\n",
        "user_id_mapping = {id:i for i, id in enumerate(df_filtered['User'].unique())}\n",
        "movie_id_mapping = {id:i for i, id in enumerate(df_filtered['Movie'].unique())}\n",
        "\n",
        "# Create correctly mapped train- & testset\n",
        "train_user_data = X_train['User'].map(user_id_mapping)\n",
        "train_movie_data = X_train['Movie'].map(movie_id_mapping)\n",
        "\n",
        "test_user_data = X_test['User'].map(user_id_mapping)\n",
        "test_movie_data = X_test['Movie'].map(movie_id_mapping)\n",
        "\n",
        "# Get input variable-sizes\n",
        "users = len(user_id_mapping)\n",
        "movies = len(movie_id_mapping)\n",
        "embedding_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACkQet-hVit_"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3wnJ94Gw5jV",
        "outputId": "4fc70d09-2765-42da-e497-4e40be5f73c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.0346\n",
            "Epoch [2/10], Loss: 0.9365\n",
            "Epoch [3/10], Loss: 0.7360\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m         loss \u001b[39m=\u001b[39m criterion(y_pred, ratings)\n\u001b[1;32m     73\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 74\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     75\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, epochs, loss\u001b[39m.\u001b[39mitem()))\n\u001b[1;32m     77\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[0;32m~/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/optim/adam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    344\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    348\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class MatrixFactorizationDataset(Dataset):\n",
        "    def __init__(self, users, movies, ratings):\n",
        "        self.users = users\n",
        "        self.movies = movies\n",
        "        self.ratings = ratings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.movies[idx], self.ratings[idx]\n",
        "\n",
        "\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, users, movies, embedding_size):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(users, embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(movies, embedding_size)\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        user_vectors = self.user_embedding(user_ids)\n",
        "        movie_vectors = self.movie_embedding(movie_ids)\n",
        "        y = torch.sum(user_vectors * movie_vectors, dim=1)\n",
        "        return y\n",
        "\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = MatrixFactorizationDataset(train_user_data.values, train_movie_data.values, df_train['Rating'].values)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = MatrixFactorization(users, movies, embedding_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for user_ids, movie_ids, ratings in train_dataloader:\n",
        "        user_ids = user_ids.long()\n",
        "        movie_ids = movie_ids.long()\n",
        "        ratings = ratings.float()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(user_ids, movie_ids)\n",
        "        loss = criterion(y_pred, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_user_data_tensor = torch.tensor(test_user_data.values).long()\n",
        "    test_movie_data_tensor = torch.tensor(test_movie_data.values).long()\n",
        "    y_pred = model(test_user_data_tensor, test_movie_data_tensor).numpy()\n",
        "y_true = df_test['Rating'].values\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "print('\\n\\nTesting Result With PyTorch Matrix-Factorization: {:.4f} RMSE'.format(rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAHkx0Dpw5jW",
        "outputId": "cca858fd-9da8-4ed5-9aaa-c7972e1f0518"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save model to disk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodel_nn_matrix_w_gradient.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# save model to disk\n",
        "torch.save(model.state_dict(), \"model_nn_matrix_w_gradient.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUd1YFpoViuA"
      },
      "source": [
        "### Matrix Factorization (hidden layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC7UsZbYViuA"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0eZ4SBlViuA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Setup variables\n",
        "user_embedding_size = 20\n",
        "movie_embedding_size = 10\n",
        "\n",
        "# Create user- & movie-id mapping\n",
        "user_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\n",
        "movie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n",
        "\n",
        "# Create correctly mapped train- & testset\n",
        "train_user_data = df_train['User'].map(user_id_mapping)\n",
        "train_movie_data = df_train['Movie'].map(movie_id_mapping)\n",
        "\n",
        "test_user_data = df_test['User'].map(user_id_mapping)\n",
        "test_movie_data = df_test['Movie'].map(movie_id_mapping)\n",
        "\n",
        "# Get input variable-sizes\n",
        "users = len(user_id_mapping)\n",
        "movies = len(movie_id_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dtzC8xGViuA"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGhtk9MVw5jW",
        "outputId": "2dd41c76-febf-497a-db06-a557740c4e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Testing Result With PyTorch Deep Learning: 0.9142 RMSE\n"
          ]
        }
      ],
      "source": [
        "class Recommender(nn.Module):\n",
        "    def __init__(self, user_embedding_size, movie_embedding_size, users, movies):\n",
        "        super(Recommender, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(users, user_embedding_size)\n",
        "        self.movie_embedding = nn.Embedding(movies, movie_embedding_size)\n",
        "        self.fc1 = nn.Linear(user_embedding_size + movie_embedding_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        user_vector = self.user_embedding(user_ids)\n",
        "        movie_vector = self.movie_embedding(movie_ids)\n",
        "        concat = torch.cat((user_vector, movie_vector), dim=-1)\n",
        "        dense = self.fc1(concat)\n",
        "        y = self.fc2(dense)\n",
        "        return y\n",
        "\n",
        "model = Recommender(user_embedding_size, movie_embedding_size, users, movies)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Prepare data\n",
        "train_data = TensorDataset(torch.tensor(train_user_data.values, dtype=torch.int32),\n",
        "                            torch.tensor(train_movie_data.values, dtype=torch.int32),\n",
        "                            torch.tensor(df_train['Rating'].values, dtype=torch.float))\n",
        "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# Fit model\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for _, (user_ids, movie_ids, ratings) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(user_ids, movie_ids).squeeze()\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test model\n",
        "test_data = TensorDataset(torch.tensor(test_user_data.values, dtype=torch.int32),\n",
        "                          torch.tensor(test_movie_data.values, dtype=torch.int32),\n",
        "                          torch.tensor(df_test['Rating'].values, dtype=torch.float))\n",
        "test_loader = DataLoader(test_data, batch_size=256)\n",
        "\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for _, (user_ids, movie_ids, ratings) in enumerate(test_loader):\n",
        "        predictions = model(user_ids, movie_ids).squeeze().tolist()\n",
        "        y_pred.extend(predictions)\n",
        "        y_true.extend(ratings.tolist())\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "print('\\n\\nTesting Result With PyTorch Deep Learning: {:.4f} RMSE'.format(rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YScoPxrSw5jW"
      },
      "outputs": [],
      "source": [
        "# save model to disk\n",
        "torch.save(model.state_dict(), \"model_nn_matrix_w_gradient.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpXmsGjNViuA"
      },
      "source": [
        "### The Deep Hybrid System with Metadata\n",
        "Uses movie metadata to improve recommendations. Currently only uses tf-idf vectorizations of descriptions (might swap/add tf-idf keywords in future). Metadata is combined with embeddings of user-id and movie-id. Reduces cold-start problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbqmum-hViuA"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiIWJVMCViuA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.sparse import vstack\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg9bX8JyViuB"
      },
      "source": [
        "A faster tensor data loader to batch load and speed up tabular data loading. Modified from source to support sparse tensors.\n",
        "\n",
        "Source: https://github.com/hcarlens/pytorch-tabular/blob/master/fast_tensor_data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssE0p-UZViuB",
        "outputId": "beeb66f3-7d69-4de6-cea3-c58d11d9f660"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_filterd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create user- & movie-id mapping\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m user_id_mapping \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m:i \u001b[39mfor\u001b[39;00m i, \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df_filterd[\u001b[39m'\u001b[39m\u001b[39mUser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())}\n\u001b[1;32m      3\u001b[0m movie_id_mapping \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m:i \u001b[39mfor\u001b[39;00m i, \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df_filterd[\u001b[39m'\u001b[39m\u001b[39mMovie\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())}\n\u001b[1;32m      5\u001b[0m \u001b[39m# Use mapping to get better ids\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_filterd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create user- & movie-id mapping\n",
        "user_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\n",
        "movie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n",
        "\n",
        "# Use mapping to get better ids\n",
        "df_filterd['User'] = df_filterd['User'].map(user_id_mapping)\n",
        "df_filterd['Movie'] = df_filterd['Movie'].map(movie_id_mapping)\n",
        "\n",
        "\n",
        "##### Combine both datasets to get movies with metadata\n",
        "# Preprocess metadata\n",
        "tmp_metadata = movie_metadata.copy()\n",
        "tmp_metadata.index = tmp_metadata.index.str.lower()\n",
        "\n",
        "# Preprocess titles\n",
        "tmp_titles = movie_titles.drop('Year', axis=1).copy()\n",
        "tmp_titles = tmp_titles.reset_index().set_index('Name')\n",
        "tmp_titles.index = tmp_titles.index.str.lower()\n",
        "\n",
        "# Combine titles and metadata\n",
        "df_id_descriptions = tmp_titles.join(tmp_metadata).dropna().set_index('Id')\n",
        "df_id_descriptions['description'] = df_id_descriptions['description'].str.lower()\n",
        "del tmp_metadata,tmp_titles\n",
        "\n",
        "# Filter all ratings with metadata\n",
        "df_hybrid = df_filterd.set_index('Movie').join(df_id_descriptions).dropna().drop('description', axis=1).reset_index().rename({'index':'Movie'}, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr4RCTgRViuB"
      },
      "outputs": [],
      "source": [
        "# Split train- & testset\n",
        "n = 100000\n",
        "df_hybrid = df_hybrid.sample(frac=1).reset_index(drop=True)\n",
        "df_hybrid_train = df_hybrid[:1500000]\n",
        "df_hybrid_test = df_hybrid[-n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8x5XYNViuB"
      },
      "outputs": [],
      "source": [
        "# Create tf-idf matrix for text comparison\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_hybrid = tfidf.fit_transform(df_id_descriptions['description'])\n",
        "\n",
        "# Get mapping from movie-ids to indices in tfidf-matrix\n",
        "mapping = {id:i for i, id in enumerate(df_id_descriptions.index)}\n",
        "\n",
        "train_tfidf = []\n",
        "# Iterate over all movie-ids and save the tfidf-vector\n",
        "for id in df_hybrid_train['Movie'].values:\n",
        "    index = mapping[id]\n",
        "    train_tfidf.append(tfidf_hybrid[index])\n",
        "    \n",
        "test_tfidf = []\n",
        "# Iterate over all movie-ids and save the tfidf-vector\n",
        "for id in df_hybrid_test['Movie'].values:\n",
        "    index = mapping[id]\n",
        "    test_tfidf.append(tfidf_hybrid[index])\n",
        "\n",
        "# Stack the sparse matrices\n",
        "train_tfidf = vstack(train_tfidf)\n",
        "test_tfidf = vstack(test_tfidf)\n",
        "\n",
        "# TODO: Test if can remove below\n",
        "# Create dense numpy arrays (might not need)\n",
        "# tfidf_hybrid = tfidf_hybrid.toarray() # Convert to dense array\n",
        "\n",
        "\n",
        "# # Get mapping from movie-ids to indices in tfidf-matrix\n",
        "# mapping = {id:i for i, id in enumerate(df_id_descriptions.index)}\n",
        "\n",
        "# train_tfidf = np.array([tfidf_hybrid[mapping[id]] for id in df_hybrid_train['Movie'].values]) # Dense tensor array\n",
        "# test_tfidf = np.array([tfidf_hybrid[mapping[id]] for id in df_hybrid_test['Movie'].values]) # Dense tensor array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85RksEuQViuB"
      },
      "source": [
        "Create Datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vOhW8HxViuB",
        "outputId": "bc17f70c-3115-47ca-838e-0baa81209347"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FastTensorDataLoader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO: Remove commented\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#train_dataset = HybridDataset(df_hybrid_train['User'].values, df_hybrid_train['Movie'].values, sparse_to_torch_sparse(train_tfidf), df_hybrid_train['Rating'].values)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[39m# sparse_to_torch_sparse(train_tfidf), \u001b[39;00m\n\u001b[1;32m      9\u001b[0m                               \u001b[39m# torch.FloatTensor(df_hybrid_train['Rating'].values))\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_dataloader \u001b[39m=\u001b[39m FastTensorDataLoader(torch\u001b[39m.\u001b[39mtensor(df_hybrid_train[\u001b[39m'\u001b[39m\u001b[39mUser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong), \n\u001b[1;32m     11\u001b[0m                               torch\u001b[39m.\u001b[39mtensor(df_hybrid_train[\u001b[39m'\u001b[39m\u001b[39mMovie\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong), \n\u001b[1;32m     12\u001b[0m                               sparse_to_torch_sparse(train_tfidf), \n\u001b[1;32m     13\u001b[0m                               torch\u001b[39m.\u001b[39mFloatTensor(df_hybrid_train[\u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues), batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[39m#test_dataset = TensorDataset(torch.tensor(df_hybrid_test['User'].values, dtype=torch.long), \u001b[39;00m\n\u001b[1;32m     15\u001b[0m                             \u001b[39m#  torch.tensor(df_hybrid_test['Movie'].values, dtype=torch.long), \u001b[39;00m\n\u001b[1;32m     16\u001b[0m                             \u001b[39m#  sparse_to_torch_sparse(test_tfidf), \u001b[39;00m\n\u001b[1;32m     17\u001b[0m                             \u001b[39m#  torch.FloatTensor(df_hybrid_test['Rating'].values))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m test_dataloader \u001b[39m=\u001b[39m FastTensorDataLoader(torch\u001b[39m.\u001b[39mtensor(df_hybrid_test[\u001b[39m'\u001b[39m\u001b[39mUser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong), \n\u001b[1;32m     19\u001b[0m                              torch\u001b[39m.\u001b[39mtensor(df_hybrid_test[\u001b[39m'\u001b[39m\u001b[39mMovie\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong), \n\u001b[1;32m     20\u001b[0m                              sparse_to_torch_sparse(test_tfidf), \n\u001b[1;32m     21\u001b[0m                              torch\u001b[39m.\u001b[39mFloatTensor(df_hybrid_test[\u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues), batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastTensorDataLoader' is not defined"
          ]
        }
      ],
      "source": [
        "# TODO: Remove commented\n",
        "\n",
        "\n",
        "#train_dataset = HybridDataset(df_hybrid_train['User'].values, df_hybrid_train['Movie'].values, sparse_to_torch_sparse(train_tfidf), df_hybrid_train['Rating'].values)\n",
        "#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#test_dataset = HybridDataset(df_hybrid_test['User'].values, df_hybrid_test['Movie'].values, sparse_to_torch_sparse(test_tfidf), df_hybrid_test['Rating'].values)\n",
        "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "#train_dataset = TensorDataset(torch.tensor(df_hybrid_train['User'].values, dtype=torch.long), \n",
        "                              # torch.tensor(df_hybrid_train['Movie'].values, dtype=torch.long), \n",
        "                              # sparse_to_torch_sparse(train_tfidf), \n",
        "                              # torch.FloatTensor(df_hybrid_train['Rating'].values))\n",
        "train_dataloader = FastTensorDataLoader(torch.tensor(df_hybrid_train['User'].values, dtype=torch.long), \n",
        "                              torch.tensor(df_hybrid_train['Movie'].values, dtype=torch.long), \n",
        "                              sparse_to_torch_sparse(train_tfidf), \n",
        "                              torch.FloatTensor(df_hybrid_train['Rating'].values), batch_size=batch_size, shuffle=True)\n",
        "#test_dataset = TensorDataset(torch.tensor(df_hybrid_test['User'].values, dtype=torch.long), \n",
        "                            #  torch.tensor(df_hybrid_test['Movie'].values, dtype=torch.long), \n",
        "                            #  sparse_to_torch_sparse(test_tfidf), \n",
        "                            #  torch.FloatTensor(df_hybrid_test['Rating'].values))\n",
        "test_dataloader = FastTensorDataLoader(torch.tensor(df_hybrid_test['User'].values, dtype=torch.long), \n",
        "                             torch.tensor(df_hybrid_test['Movie'].values, dtype=torch.long), \n",
        "                             sparse_to_torch_sparse(test_tfidf), \n",
        "                             torch.FloatTensor(df_hybrid_test['Rating'].values), batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDHjXoVViuB"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA4NkFwFViuB"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "user_embed_dim = 10\n",
        "movie_embed_dim = 10\n",
        "tfidf_dim = train_tfidf.shape[1]\n",
        "num_users = len(user_id_mapping)\n",
        "num_movies = len(movie_id_mapping)\n",
        "epochs = 100\n",
        "batch_size = 256\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmxyYGGfw5jX",
        "outputId": "cd610a4c-a039-4e66-e15c-246dad21b9bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gmbaker3/CSC-422-Netflix-Recommendations-Web-Scrapping-Ensemble-Models/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning:\n",
            "\n",
            "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Network definition\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_users, num_movies, user_embed_dim, movie_embed_dim, tfidf_dim):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, user_embed_dim)\n",
        "        self.movie_embedding = nn.Embedding(num_movies, movie_embed_dim)\n",
        "        self.fc1 = nn.Linear(tfidf_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(user_embed_dim + movie_embed_dim + 32, 512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc4 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, user_ids, movie_ids, tfidf_vectors):\n",
        "        user_embed = self.user_embedding(user_ids)\n",
        "        movie_embed = self.movie_embedding(movie_ids)\n",
        "        x = torch.relu(self.fc1(tfidf_vectors))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.cat((user_embed, movie_embed, x), dim=-1)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc4(x)\n",
        "        return out.squeeze()\n",
        "\n",
        "# Training and testing\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        user_ids, movie_ids, tfidf_vectors, ratings = batch\n",
        "        user_ids = user_ids.to(device)\n",
        "        movie_ids = movie_ids.to(device)\n",
        "        tfidf_vectors = tfidf_vectors.to(device)\n",
        "        ratings = ratings.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(user_ids, movie_ids, tfidf_vectors)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def test(model, dataloader, device):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            user_ids, movie_ids, tfidf_vectors, ratings = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            movie_ids = movie_ids.to(device)\n",
        "            tfidf_vectors = tfidf_vectors.to(device)\n",
        "\n",
        "            predictions = model(user_ids, movie_ids, tfidf_vectors)\n",
        "            y_pred.extend(predictions.tolist())\n",
        "            y_true.extend(ratings.tolist())\n",
        "    \n",
        "    return y_pred, y_true\n",
        "\n",
        "# Convert sparse matrix to a PyTorch sparse tensor\n",
        "def sparse_to_torch_sparse(data):\n",
        "    values = data.data\n",
        "    indices = np.vstack((data.nonzero()[0], data.nonzero()[1]))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = data.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = HybridModel(num_users, num_movies, user_embed_dim, movie_embed_dim, tfidf_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    y_pred, y_true = test(model, test_dataloader, device)\n",
        "    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Test the model\n",
        "y_pred, y_true = test(model, test_dataloader, device)\n",
        "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "print(f'\\nTesting Result With PyTorch Hybrid Deep Learning: {rmse:.4f} RMSE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7XWqlATw5jX"
      },
      "outputs": [],
      "source": [
        "# save model to disk\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/CSC 422/CSC422 Class Project/codes/checkpoints/model_nn_hybrid_w_metadata.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgucful-gkQ"
      },
      "source": [
        "#### TPU Version (not currently using) (only works with dense tensors (need a tonnnnnnn of memory))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6huKk60ViuC"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIlDgS9U-rJk",
        "outputId": "d5156664-bfb7-4987-e03a-bf86407bea09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-xla==2.0\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl (115.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision==0.15.1 in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.9/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (1.24.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (8.4.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.21.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.17.1)\n",
            "Collecting google-api-core<2dev,>=1.13.0\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.1.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from torch-xla==2.0) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0) (2.1.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.59.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (5.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.9)\n",
            "Installing collected packages: uritemplate, google-api-core, google-api-python-client, cloud-tpu-client, torch-xla\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.70.0\n",
            "    Uninstalling google-api-python-client-2.70.0:\n",
            "      Successfully uninstalled google-api-python-client-2.70.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.347 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloud-tpu-client-0.10 google-api-core-1.34.0 google-api-python-client-1.8.0 torch-xla-2.0 uritemplate-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnrW2D-DViuC"
      },
      "source": [
        "##### Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "pEJ61Gbw-gLe",
        "outputId": "098952ea-0322-4adf-c7c5-91cfd2646e74"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a97fb7862107>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a custom dataset for DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHybridDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_xla'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "# Create a custom dataset for DataLoader\n",
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, users, movies, tfidf, ratings):\n",
        "        self.users = torch.tensor(users, dtype=torch.long)\n",
        "        self.movies = torch.tensor(movies, dtype=torch.long)\n",
        "        self.tfidf = tfidf\n",
        "        self.ratings = torch.FloatTensor(ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tfidf_vector = self.tfidf[idx].squeeze()\n",
        "        return self.users[idx], self.movies[idx], tfidf_vector, self.ratings[idx]\n",
        "\n",
        "# Network definition\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_users, num_movies, user_embed_dim, movie_embed_dim, tfidf_dim):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, user_embed_dim)\n",
        "        self.movie_embedding = nn.Embedding(num_movies, movie_embed_dim)\n",
        "        self.fc1 = nn.Linear(tfidf_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(user_embed_dim + movie_embed_dim + 32, 512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc4 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, user_ids, movie_ids, tfidf_vectors):\n",
        "        user_embed = self.user_embedding(user_ids)\n",
        "        movie_embed = self.movie_embedding(movie_ids)\n",
        "        x = torch.relu(self.fc1(tfidf_vectors))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.cat((user_embed, movie_embed, x), dim=-1)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc4(x)\n",
        "        return out.squeeze()\n",
        "\n",
        "# Training and testing\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        user_ids, movie_ids, tfidf_vectors, ratings = batch\n",
        "        user_ids = user_ids.to(device)\n",
        "        movie_ids = movie_ids.to(device)\n",
        "        tfidf_vectors = tfidf_vectors.to(device)\n",
        "        ratings = ratings.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(user_ids, movie_ids, tfidf_vectors)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def test(model, dataloader, device):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            user_ids, movie_ids, tfidf_vectors, ratings = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            movie_ids = movie_ids.to(device)\n",
        "            tfidf_vectors = tfidf_vectors.to(device)\n",
        "\n",
        "            predictions = model(user_ids, movie_ids, tfidf_vectors)\n",
        "            y_pred.extend(predictions.tolist())\n",
        "            y_true.extend(ratings.tolist())\n",
        "    \n",
        "    return y_pred, y_true\n",
        "\n",
        "# Convert sparse matrix to a PyTorch sparse tensor\n",
        "def sparse_to_torch_sparse(data):\n",
        "    values = data.data\n",
        "    indices = np.vstack((data.nonzero()[0], data.nonzero()[1]))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = data.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "# Set device\n",
        "device = xm.xla_device()\n",
        "\n",
        "# Hyperparameters\n",
        "user_embed_dim = 10\n",
        "movie_embed_dim = 10\n",
        "tfidf_dim = train_tfidf.shape[1]\n",
        "num_users = len(user_id_mapping)\n",
        "num_movies = len(movie_id_mapping)\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = HybridDataset(df_hybrid_train['User'].values, df_hybrid_train['Movie'].values, sparse_to_torch_sparse(train_tfidf), df_hybrid_train['Rating'].values)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = HybridDataset(df_hybrid_test['User'].values, df_hybrid_test['Movie'].values, sparse_to_torch_sparse(test_tfidf), df_hybrid_test['Rating'].values)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = HybridModel(num_users, num_movies, user_embed_dim, movie_embed_dim, tfidf_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    y_pred, y_true = test(model, test_dataloader, device)\n",
        "    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "    xm.rendezvous('sync_epoch')\n",
        "    xm.mark_step()\n",
        "\n",
        "# Test the model\n",
        "y_pred, y_true = test(model, test_dataloader, device)\n",
        "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
        "print(f'\\nTesting Result With PyTorch Hybrid Deep Learning: {rmse:.4f} RMSE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN9Z_kJNViuC"
      },
      "source": [
        "##### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SfvVAOrCadM",
        "outputId": "926e1d03-e1ac-494d-f9cd-092e99a01fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity ratio: 99.91%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a dense PyTorch tensor\n",
        "dense_tensor = torch.from_numpy(train_tfidf.toarray())\n",
        "\n",
        "# Calculate the number of non-zero elements in the tensor\n",
        "num_nonzero = torch.nonzero(dense_tensor).size(0)\n",
        "\n",
        "# Calculate the total number of elements in the tensor\n",
        "total_elements = dense_tensor.numel()\n",
        "\n",
        "# Calculate the sparsity ratio\n",
        "sparsity_ratio = 1.0 - (num_nonzero / total_elements)\n",
        "\n",
        "# Print the sparsity ratio\n",
        "print(\"Sparsity ratio: {:.2f}%\".format(sparsity_ratio * 100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}